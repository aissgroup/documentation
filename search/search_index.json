{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"01_business_plan/demo_video/","text":"Demo Video of the QSROA In the following you can see our introduction-video of the QSROA:","title":"QSROA - Demo Video"},{"location":"01_business_plan/demo_video/#demo-video-of-the-qsroa","text":"In the following you can see our introduction-video of the QSROA:","title":"Demo Video of the QSROA"},{"location":"01_business_plan/use_case/","text":"Quick Service Restaurant Order Assistant (QSROA) - Business Plan Mission Statement We, Christian, Felix, Julian, Joshua, Leon and Magnus are a group of students, all with a fascination in machine learning and a willingness not only to learn about ML techniques but also to \"get our hands dirty\" and use our theoretical knowledge and expand it practically. As participants of the AISS-CV (Artificial Intelligence in Service Systems - Computer Vision) course, we had the task to invent a Computer Vision service solution deployable on a \"Jetson Nano\". Put another way, we had to develop a service running on a computer attached to a camera that detects objects in real-time with the help of a neural network. Quickly we realized that our invention could be a game-changer for our potential customer market. In the following analysis, we will elaborate on this. Our overall goal is to create profit for our customers as well as for their customers - or to say in a prosaic way \"make our customers happy by making their customers happy\". Breaking down our overall goal to a more fine-granular level, we will examine the service we provide, the primary market where the most profit with our service can be made and, where for now our geographical region of operations will be. We invented an AI-order-assistant for Quick Service Restaurants to check the completeness of an order. Incorrect order composition is not only a major source of dissatisfaction for customers eating at the restaurant - especially in-home deliveries the damage done is significantly higher to both the restaurant and the customer. On the customer's side, the level of frustration rises as the correction of the faulty delivery takes time. On the restaurant side, the company now not only has to deal with preparing the missing products - both an additional material and time expense, it additionally has to deliver those as quickly as possible to the customer, a rather costly process. Like that isn't enough, the damage done to the company's image grows with every minute the customer has to wait for her correct order. With QSROA we intend to eliminate the occurrence of this inconvenient process once and for all! In times where the market for food delivery is on the rise ( Revenue Germany Food-Delivery 2020: 4.0 billion \u20ac, 2025: estimated 6.9 billion \u20ac ), our solution can save restaurants from high costs. At the moment, we see our solution mainly in the area of large quick service restaurant chains, as there is a maximum standardization of the menu (packaging) and we believe that we can \"make our customers happy by making their customers happy\" the fastest here. As we constructed our service always trying to keep it as simple as possible - we believe we aren't restricted geographically, neither do we have to adapt much to geographically different market situations. Business Description Many quick-service restaurants, especially in the fast-food industry are facing a similar challenge - large amounts of orders hit employees with time being a limited and valuable factor. These orders can reach high levels of complexity and contain a vast variety of products. As a result, selecting and packing the right menu items (now simply called items) turns into a crucial and error-prone task that requires a constantly high level of focus. Mistakes made typically result in costly fixes, a lowered customer satisfaction and reputation damages in the long run. The Quick Service Restaurant Order Assistant (QSROA) developed in the course of this lecture, therefore, aims to help employees through the order handling process by verifying the status of the current order. In addition, the time-consuming process of frequently re-reading the order on receipts can be minimized and thus orders can be processed faster. Using a camera surveying the packing area, the assistant continuously collates the detected items with the current order. QSROA thereby guides the employee not only towards missing items but also wrong ones and verifies the status of the order. We as a group want to create value to the market by entering into collaborations with our business partners, not presenting them a fixed product, but rather adapt our service to their needs and overcome different chain-specific challenges in direct discourse. Our solution can be broken down into the following main parts: Hardware: an edge device with an attached camera Software: real-time menu item detection running on the edge device web-based interface collating detected items with the current order Service: software maintenance updating network corresponding to seasonal menu-changes future work: bottleneck detection - tracking how long it takes to produce certain menu items. To get a better system overview, in the following a simplified representation of QSROA in the field of application is shown. The system consists of three main parts. A camera module[1] and a monitor[2] connected to a Nvidia Jetson Nano, a packing area[3] and an employee[4] interacting with the QSROA. Once a new order is placed in the system, the monitor displays its requested items to the employee. The job of the employee is now to collect the required items and place them in the packing area. The camera captures the items and sends the image to the Jetson Nano, where a custom-trained neural network is working to detect the items/objects in the image. The items are then transferred to the new status \"checked off\". As soon as all of the items have been detected, the order can be marked as complete and the employee may pack the ordered menu and proceed with the following menu request. In conclusion, our system serves as an easy-to-implement, additional verification instance capable of significantly reducing costs caused by human failure in the processing of orders. Deployment and Maintenance One of the key features of the QSROA is that the whole system can be run on a single Jetson Nano. In order to put the system into use the customer essentially only needs to install the device and create a connection to a display as well as the customer's checkout system. The establishment of a connection to the checkout system represents the most complex step in the deployment as it most likely requires a modification in the customer's system. However, as our customers are forecasted to be mostly large chains, a standardization and therefore simplification of this process can be expected. The simplicity of the assistant continues in the dataset used for training the neural network. The training data largely/purely consists of synthetic data, thereby needing relatively few of it. This helps to provide a very convenient maintenance process as the model of the QSROA can be retrained easily and cost-effectively, again only needing few data. Thus, the assistant can react especially well to minor or seasonal changes in the restaurant's menu. Impact for Potential QSROA-Customers Potential customers range from large quick service restaurant chains to small company cafeterias. They all share the problem of occasionally trying to avoid employees having to perform monotonous work for long periods of time, which is often error-prone in the long run and can for example end up in packing orders that don't fulfill what the customer asked for. This problem becomes even more severe in situations where orders cannot be fixed after leaving the kitchen, namely in food deliveries. The costs of faulty deliveries are significant. Not only do restaurants have to send out another delivery, but they may also lose the goodwill of the customer thereby potentially reducing future orders. Especially the potential damage in reputation is not to be neglected. As a saying goes, \" It takes years to build a positive image, but only five minutes to ruin it\" . With our new assistant, QSROA customers can take another step towards preserving their hard-earned reputation by further minimizing situations that may cause customer dissatisfaction. Monotonous work not only dissatisfies employees and as a result motivates them to look for new job opportunities, but also can negatively impact mental health ( Judy Willis 2014 ) and lead to error-prone work performances. Since it is in the interest of a company to avoid employees dropping out for an unpredictable time, it is important to restrict monotonous workflows. By implementing the QSROA into a kitchen workflow, it is also possible to document event-logs/timestamps of the ordered items when they are ready for packing. With process-mining techniques, it is then possible to detect and visualize bottlenecks in the kitchen workflow and further optimize the process to reduce the overall menu delivery time. SWOT Analysis Strengths: agile development-team simple solution relatively easy to large scale our service (country-wide) Weaknesses: Little knowledge in quick-order restaurant domain No full-time development (students) Opportunities: Growing food-delivery market Threats: High pressure due to easy replaceability in early-stage development (less domain knowledge) Industry Background Quick service restaurants (QSR)-Industry: There are many different quick service restaurant brands like Burger King, McDonald's, Starbucks, Taco Bell, etc.. In the U.S. alone, this industry generated revenue of 239 billion U.S. dollars in 2020 ( S. Lock (2020) ). The Food-Delivery-Industry on the other hand generated 28 billion U.S. dollars of revenue in the U.S. in 2020 ( Statistica ). For now we will not focus on limited service restaurants (LSR), because it is easier to large-scale our solution for standardized restaurant-chains. As the markets for QSR and food deliveries are growing constantly not only in the U.S. but also worldwide, it is essential to build an easy-to-implement solution to serve this growing market. Computer Vision Research: Since we expect further growth in the field of computer vision, we can also expect more advanced techniques to improve our service. Competitor Analysis Our biggest competitor is probably auvisus but they for now have a different scope - they focus on canteens, not on QSR chains. Therefore, we will not have any problems since we operate in different markets. Market Analysis In the early stages, our product exclusively targets large QSR chains. Their rare menu changes as well as internally motivated high product recognizability allows for a large-scale deployment with a low need for individualization. We reckon with a sales potential of over 1.000 devices within the first three years, as BurgerKing and McDonalds already operate over 57.000 restaurants worldwide. Since our product is barely affected by regional factors, we believe that every single one of the restaurants could potentially be equipped with one or more QSROAs, depending on their size. In addition, with the turnover in food deliveries presumably doubling between 2019 and 2025 ( Online Food Delivery - worldwide ), the need for accurate delivery processes and assisting technologies will increase even further. While focusing at first on the large QSR companies comes with several advantages, it also connects QSROA to the problems of \"classic\" QSR chains, namely the shifting perspective of customers towards healthier and more eco-friendly products. However, as the demand for fast food itself does not decrease, we see ourselves very much in the position to extend our target audience after the initial phase to new emerging QSR companies. Outlook While the QSROA can be a value-adding instrument in its initial use already, there are potential further scenarios in which the QSROA may deliver value to customers. With the integration of the QSROA into a kitchen workflow, it is possible to document the event logs/timestamps of ordered items once they are placed in the scope of the QSROA and ready for packing. With the time tracking our customers have the ability to use process mining tools to detect bottlenecks and improve their kitchen processes to reduce the overall menu delivery time. The most prominent use case appears to be in a self-serving environment. Essentially, the QSROA could be used as a verifying instance to check whether customers pick the products they paid for. In an even more advanced version, the customer's bill would be based upon the products the customer picked and the QSROA detected. In both scenarios the need for employees would significantly decrease, hence increasing our customer's competitiveness in the market through reduced costs and increased customer satisfaction.","title":"Business Plan"},{"location":"01_business_plan/use_case/#quick-service-restaurant-order-assistant-qsroa-business-plan","text":"","title":"Quick Service Restaurant Order Assistant (QSROA) - Business Plan"},{"location":"01_business_plan/use_case/#mission-statement","text":"We, Christian, Felix, Julian, Joshua, Leon and Magnus are a group of students, all with a fascination in machine learning and a willingness not only to learn about ML techniques but also to \"get our hands dirty\" and use our theoretical knowledge and expand it practically. As participants of the AISS-CV (Artificial Intelligence in Service Systems - Computer Vision) course, we had the task to invent a Computer Vision service solution deployable on a \"Jetson Nano\". Put another way, we had to develop a service running on a computer attached to a camera that detects objects in real-time with the help of a neural network. Quickly we realized that our invention could be a game-changer for our potential customer market. In the following analysis, we will elaborate on this. Our overall goal is to create profit for our customers as well as for their customers - or to say in a prosaic way \"make our customers happy by making their customers happy\". Breaking down our overall goal to a more fine-granular level, we will examine the service we provide, the primary market where the most profit with our service can be made and, where for now our geographical region of operations will be. We invented an AI-order-assistant for Quick Service Restaurants to check the completeness of an order. Incorrect order composition is not only a major source of dissatisfaction for customers eating at the restaurant - especially in-home deliveries the damage done is significantly higher to both the restaurant and the customer. On the customer's side, the level of frustration rises as the correction of the faulty delivery takes time. On the restaurant side, the company now not only has to deal with preparing the missing products - both an additional material and time expense, it additionally has to deliver those as quickly as possible to the customer, a rather costly process. Like that isn't enough, the damage done to the company's image grows with every minute the customer has to wait for her correct order. With QSROA we intend to eliminate the occurrence of this inconvenient process once and for all! In times where the market for food delivery is on the rise ( Revenue Germany Food-Delivery 2020: 4.0 billion \u20ac, 2025: estimated 6.9 billion \u20ac ), our solution can save restaurants from high costs. At the moment, we see our solution mainly in the area of large quick service restaurant chains, as there is a maximum standardization of the menu (packaging) and we believe that we can \"make our customers happy by making their customers happy\" the fastest here. As we constructed our service always trying to keep it as simple as possible - we believe we aren't restricted geographically, neither do we have to adapt much to geographically different market situations.","title":"Mission Statement"},{"location":"01_business_plan/use_case/#business-description","text":"Many quick-service restaurants, especially in the fast-food industry are facing a similar challenge - large amounts of orders hit employees with time being a limited and valuable factor. These orders can reach high levels of complexity and contain a vast variety of products. As a result, selecting and packing the right menu items (now simply called items) turns into a crucial and error-prone task that requires a constantly high level of focus. Mistakes made typically result in costly fixes, a lowered customer satisfaction and reputation damages in the long run. The Quick Service Restaurant Order Assistant (QSROA) developed in the course of this lecture, therefore, aims to help employees through the order handling process by verifying the status of the current order. In addition, the time-consuming process of frequently re-reading the order on receipts can be minimized and thus orders can be processed faster. Using a camera surveying the packing area, the assistant continuously collates the detected items with the current order. QSROA thereby guides the employee not only towards missing items but also wrong ones and verifies the status of the order. We as a group want to create value to the market by entering into collaborations with our business partners, not presenting them a fixed product, but rather adapt our service to their needs and overcome different chain-specific challenges in direct discourse. Our solution can be broken down into the following main parts: Hardware: an edge device with an attached camera Software: real-time menu item detection running on the edge device web-based interface collating detected items with the current order Service: software maintenance updating network corresponding to seasonal menu-changes future work: bottleneck detection - tracking how long it takes to produce certain menu items. To get a better system overview, in the following a simplified representation of QSROA in the field of application is shown. The system consists of three main parts. A camera module[1] and a monitor[2] connected to a Nvidia Jetson Nano, a packing area[3] and an employee[4] interacting with the QSROA. Once a new order is placed in the system, the monitor displays its requested items to the employee. The job of the employee is now to collect the required items and place them in the packing area. The camera captures the items and sends the image to the Jetson Nano, where a custom-trained neural network is working to detect the items/objects in the image. The items are then transferred to the new status \"checked off\". As soon as all of the items have been detected, the order can be marked as complete and the employee may pack the ordered menu and proceed with the following menu request. In conclusion, our system serves as an easy-to-implement, additional verification instance capable of significantly reducing costs caused by human failure in the processing of orders.","title":"Business Description"},{"location":"01_business_plan/use_case/#deployment-and-maintenance","text":"One of the key features of the QSROA is that the whole system can be run on a single Jetson Nano. In order to put the system into use the customer essentially only needs to install the device and create a connection to a display as well as the customer's checkout system. The establishment of a connection to the checkout system represents the most complex step in the deployment as it most likely requires a modification in the customer's system. However, as our customers are forecasted to be mostly large chains, a standardization and therefore simplification of this process can be expected. The simplicity of the assistant continues in the dataset used for training the neural network. The training data largely/purely consists of synthetic data, thereby needing relatively few of it. This helps to provide a very convenient maintenance process as the model of the QSROA can be retrained easily and cost-effectively, again only needing few data. Thus, the assistant can react especially well to minor or seasonal changes in the restaurant's menu.","title":"Deployment and Maintenance"},{"location":"01_business_plan/use_case/#impact-for-potential-qsroa-customers","text":"Potential customers range from large quick service restaurant chains to small company cafeterias. They all share the problem of occasionally trying to avoid employees having to perform monotonous work for long periods of time, which is often error-prone in the long run and can for example end up in packing orders that don't fulfill what the customer asked for. This problem becomes even more severe in situations where orders cannot be fixed after leaving the kitchen, namely in food deliveries. The costs of faulty deliveries are significant. Not only do restaurants have to send out another delivery, but they may also lose the goodwill of the customer thereby potentially reducing future orders. Especially the potential damage in reputation is not to be neglected. As a saying goes, \" It takes years to build a positive image, but only five minutes to ruin it\" . With our new assistant, QSROA customers can take another step towards preserving their hard-earned reputation by further minimizing situations that may cause customer dissatisfaction. Monotonous work not only dissatisfies employees and as a result motivates them to look for new job opportunities, but also can negatively impact mental health ( Judy Willis 2014 ) and lead to error-prone work performances. Since it is in the interest of a company to avoid employees dropping out for an unpredictable time, it is important to restrict monotonous workflows. By implementing the QSROA into a kitchen workflow, it is also possible to document event-logs/timestamps of the ordered items when they are ready for packing. With process-mining techniques, it is then possible to detect and visualize bottlenecks in the kitchen workflow and further optimize the process to reduce the overall menu delivery time.","title":"Impact for Potential QSROA-Customers"},{"location":"01_business_plan/use_case/#swot-analysis","text":"Strengths: agile development-team simple solution relatively easy to large scale our service (country-wide) Weaknesses: Little knowledge in quick-order restaurant domain No full-time development (students) Opportunities: Growing food-delivery market Threats: High pressure due to easy replaceability in early-stage development (less domain knowledge)","title":"SWOT Analysis"},{"location":"01_business_plan/use_case/#industry-background","text":"Quick service restaurants (QSR)-Industry: There are many different quick service restaurant brands like Burger King, McDonald's, Starbucks, Taco Bell, etc.. In the U.S. alone, this industry generated revenue of 239 billion U.S. dollars in 2020 ( S. Lock (2020) ). The Food-Delivery-Industry on the other hand generated 28 billion U.S. dollars of revenue in the U.S. in 2020 ( Statistica ). For now we will not focus on limited service restaurants (LSR), because it is easier to large-scale our solution for standardized restaurant-chains. As the markets for QSR and food deliveries are growing constantly not only in the U.S. but also worldwide, it is essential to build an easy-to-implement solution to serve this growing market. Computer Vision Research: Since we expect further growth in the field of computer vision, we can also expect more advanced techniques to improve our service.","title":"Industry Background"},{"location":"01_business_plan/use_case/#competitor-analysis","text":"Our biggest competitor is probably auvisus but they for now have a different scope - they focus on canteens, not on QSR chains. Therefore, we will not have any problems since we operate in different markets.","title":"Competitor Analysis"},{"location":"01_business_plan/use_case/#market-analysis","text":"In the early stages, our product exclusively targets large QSR chains. Their rare menu changes as well as internally motivated high product recognizability allows for a large-scale deployment with a low need for individualization. We reckon with a sales potential of over 1.000 devices within the first three years, as BurgerKing and McDonalds already operate over 57.000 restaurants worldwide. Since our product is barely affected by regional factors, we believe that every single one of the restaurants could potentially be equipped with one or more QSROAs, depending on their size. In addition, with the turnover in food deliveries presumably doubling between 2019 and 2025 ( Online Food Delivery - worldwide ), the need for accurate delivery processes and assisting technologies will increase even further. While focusing at first on the large QSR companies comes with several advantages, it also connects QSROA to the problems of \"classic\" QSR chains, namely the shifting perspective of customers towards healthier and more eco-friendly products. However, as the demand for fast food itself does not decrease, we see ourselves very much in the position to extend our target audience after the initial phase to new emerging QSR companies.","title":"Market Analysis"},{"location":"01_business_plan/use_case/#outlook","text":"While the QSROA can be a value-adding instrument in its initial use already, there are potential further scenarios in which the QSROA may deliver value to customers. With the integration of the QSROA into a kitchen workflow, it is possible to document the event logs/timestamps of ordered items once they are placed in the scope of the QSROA and ready for packing. With the time tracking our customers have the ability to use process mining tools to detect bottlenecks and improve their kitchen processes to reduce the overall menu delivery time. The most prominent use case appears to be in a self-serving environment. Essentially, the QSROA could be used as a verifying instance to check whether customers pick the products they paid for. In an even more advanced version, the customer's bill would be based upon the products the customer picked and the QSROA detected. In both scenarios the need for employees would significantly decrease, hence increasing our customer's competitiveness in the market through reduced costs and increased customer satisfaction.","title":"Outlook"},{"location":"02_data_generation/data_generation/","text":"Dataset Data Generation On consideration of the field of use of the QSROA, it becomes apparent that the required object detection is limited to few types of objects with instances having an extremely high level of similarity. Deducted from this finding, we conclude that synthetic data as the main component of the dataset is the ideal way to go. It combines a variety of advantages. First, it is comparatively simple to generate a comprehensive dataset. Further, generating a synthetic dataset is a rather time- and resource-saving process. Finally, a synthetic dataset comes with a high level of flexibility since it allows for easy retraining upon menu changes. Items The dataset consists of 15 objects. 14 of them are different items purchasable at McDonald's. The 15th item is hands, because they will often be seen by the QSROA once in action. Fries Hamburger Cheeseburger Big Mac Hamburger Royal TS McRib McChicken Classic Chicken McNuggets 9er McWrap McSundae Softdrink Ketchup Mayonnaise Sauce Hands Requirements When considering the requirements for the pictures that have been taken, one must differentiate between single-item and multi-item images. Single-item images are those that show only one object per picture. Their main purpose is to use them for the creation of the synthetic dataset. Multi-item images on the other hand, show multiple objects and can be used for retraining the model. Later we will confront the model performance trained purely on our synthetic dataset with the described hybrid dataset achieved performance. For both image categories, the area for the picture taken must be well lit and the camera used needs to be positioned perpendicular to the surface on which the items are being placed. Further, in single-item images, shadows are to be avoided and there must be no hands in the way. For each item a minimum of four pictures has been taken while placing the images of the items in various positions on the surface. The following figure shows our setup requirements for taking images of the relevant objects to ensure consistency through our synthetic dataset. In multi-item images we value a realistic combination of objects. The objects may slightly intersect each other, moreover, hands are allowed as well, however, the items must remain recognizable. Each item in the picture must be labeled with a bounding box of its category. Finally, it is desired to get multiple pictures of identical compositions in different spatial arrangements. Data Augmentation The exact number of single-item images used for the generation of the dataset can be seen in the following table. The process of the generation goes the following. For every picture the item is cut out. The resulting images are then scaled down to a predefined resolution. The resulting images are ready to be used for the synthetic dataset. For every synthetic picture, at first, we load a random background image out of a selection. Then, out of the pre-processed images, a few are randomly added to the picture. To increase diversity between images, we apply additional transformations to the items. These transformations include randomly rotating and warping the items to simulate different perspectives as well as blurring the image to simulate an out-of-focus effect. Items may overlap, however no more than 50 percent. On top of the classes, hands may be placed randomly to generate a more realistic representation of what the final system may expect. After all items have been placed, random noise is introduced to the final image. Noise includes black parts and spots over the image to further discourage the network from memorizing rather than learning from the images. Finally, the item position and size are saved. Since we initially planned to use COCO-Format, our annotations were created in this format. This means the annotation for all images from the dataset are saved in a single JSON file. Since we later switched to using a YOLO network, we had to convert the annotations into the new format. YOLO requires a txt file with the class location for every image. An example of a resulting synthetic picture can be seen in the following figure. The shown table displays the quantity of slightly different images available for the synthetic dataset. To test the data generation yourself, follow the DataGeneration.ipynb notebook. Item-class Quantity Fries 19 Hamburger 13 Cheeseburger 5 Big Mac 13 Hamburger Royal TS 15 McRib 7 McChicken Classic 18 Chicken McNuggets 9er 19 McWrap 24 McSundae 26 Softdrink 20 Ketchup 20 Mayonnaise 14 Sauce 28 Hands 17 Final Dataset Our final dataset consists of three different sets. A training dataset used to train the model, a validation dataset used to evaluate the model's performance while training and a test dataset to evaluate the final model performance. The training dataset consists of 45.000 synthetic images and an additional 5000 random images with no classes to further regularize the model. These 5000 images were taken from the COCO2017 dataset. The validation dataset consists of 5000 images, 95% of which are synthetic images, the remaining 5% are randomly selected stock images containing no classes. The test dataset consists of 160 real images that were labeled by hand. These pictures were taken on the system that will be deployed to customers. Distance, camera, lighting, etc. are as specified to gain information about the true real-world performance of the system. The test dataset features 146 highly complex images that may not be found in a real-world application. Classes were stacked on top of each other, were almost completely out-of-frame and include modifications such as straws in the beverages or entirely molten ice cream. This ensures that the performance we evaluate on the test set can be seen as a conservative \"lower bound\". If the system was to be shipped to a customer, we believe to be able to guarantee a performance similar to the evaluation results on the test set. However, in a day to day use a significantly better performance can be expected. Partly because the images should usually not be as complex, and in part, since we classify a continuous stream of images. This means that while a hand may block parts of an image for a couple of frames, by combining the classification results of multiple frames into our final prediction we can get a robust classification that can deal with missing out on some items every couple of frames. We originally planned to create a dataset consisting of a hybrid between synthetic data and real data. Our tests showed that adding real data into the dataset resulted in little to no improvement in model performance, as you can see in the following table. The given metrics are used to describe the accuracy of the model-output over all item-classes. Purely synthetic dataset Hybrid dataset Precision 0.950 0.932 Recall 0.822 0.844 mAP@.5 (threshold at 0.5) 0.893 0.903 mAP@.95 (threshold at 0.95) 0.650 0.641 This small performance gap might be caused by differing quantities in the two sets. While synthetic data is easy and quick to generate, real data needs to be manually annotated which is a slow and tedious process. This resulted in an imbalance between the two groups. While we used 45.000 generated synthetic images, we only added 160 real-world images. The synthetic nature of the training data ensures an unbiased and evenly distributed dataset. Human bias to items positioning, grouping or selection can be almost entirely ruled out. This also shows in the statistical analysis of the training dataset. The items are distributed evenly across the image. All classes have a similar amount of occurrences in the dataset. The width and height of the classes are less evenly distributed. This is the plain result of the shape of the McDonalds packaging and should not influence the models' performance negatively. The test dataset was created by hand. While this means that the distribution among classes is less even than with the synthetic dataset, we still tried to minimize the bias and create an evenly distributed dataset. The class mc_flurry was scratched last minute from the test dataset. This was more logistical than a technical decision since the temperature in Karlsruhe at that time transformed a McFlurry into a McMilkshake within minutes, so difficult decisions had to be made. Apart from the McFlurry, classes are roughly evenly distributed and all classes contain at least a few different samples so the model's performance can be sufficiently evaluated. Possible Extensions In a real-world environment, the synthetic dataset allows a rapid adaption of the model to an ever-changing menu. By adding images of new menu items to the synthetic data generator, new training datasets can be created instantaneously with little to no human input. Especially large QSR chains may benefit from this approach as vector graphics of the menu items and 3D models typically already exist for marketing purposes. By using these images and 3D models the generation of an even more detailed synthetic dataset may be possible. Instead of using simple 2D pixel operations, in real-world 3D renders as training data may further increase model performance by teaching the model about different perspectives and overlaps of items.","title":"Dataset"},{"location":"02_data_generation/data_generation/#dataset","text":"","title":"Dataset"},{"location":"02_data_generation/data_generation/#data-generation","text":"On consideration of the field of use of the QSROA, it becomes apparent that the required object detection is limited to few types of objects with instances having an extremely high level of similarity. Deducted from this finding, we conclude that synthetic data as the main component of the dataset is the ideal way to go. It combines a variety of advantages. First, it is comparatively simple to generate a comprehensive dataset. Further, generating a synthetic dataset is a rather time- and resource-saving process. Finally, a synthetic dataset comes with a high level of flexibility since it allows for easy retraining upon menu changes.","title":"Data Generation"},{"location":"02_data_generation/data_generation/#items","text":"The dataset consists of 15 objects. 14 of them are different items purchasable at McDonald's. The 15th item is hands, because they will often be seen by the QSROA once in action. Fries Hamburger Cheeseburger Big Mac Hamburger Royal TS McRib McChicken Classic Chicken McNuggets 9er McWrap McSundae Softdrink Ketchup Mayonnaise Sauce Hands","title":"Items"},{"location":"02_data_generation/data_generation/#requirements","text":"When considering the requirements for the pictures that have been taken, one must differentiate between single-item and multi-item images. Single-item images are those that show only one object per picture. Their main purpose is to use them for the creation of the synthetic dataset. Multi-item images on the other hand, show multiple objects and can be used for retraining the model. Later we will confront the model performance trained purely on our synthetic dataset with the described hybrid dataset achieved performance. For both image categories, the area for the picture taken must be well lit and the camera used needs to be positioned perpendicular to the surface on which the items are being placed. Further, in single-item images, shadows are to be avoided and there must be no hands in the way. For each item a minimum of four pictures has been taken while placing the images of the items in various positions on the surface. The following figure shows our setup requirements for taking images of the relevant objects to ensure consistency through our synthetic dataset. In multi-item images we value a realistic combination of objects. The objects may slightly intersect each other, moreover, hands are allowed as well, however, the items must remain recognizable. Each item in the picture must be labeled with a bounding box of its category. Finally, it is desired to get multiple pictures of identical compositions in different spatial arrangements.","title":"Requirements"},{"location":"02_data_generation/data_generation/#data-augmentation","text":"The exact number of single-item images used for the generation of the dataset can be seen in the following table. The process of the generation goes the following. For every picture the item is cut out. The resulting images are then scaled down to a predefined resolution. The resulting images are ready to be used for the synthetic dataset. For every synthetic picture, at first, we load a random background image out of a selection. Then, out of the pre-processed images, a few are randomly added to the picture. To increase diversity between images, we apply additional transformations to the items. These transformations include randomly rotating and warping the items to simulate different perspectives as well as blurring the image to simulate an out-of-focus effect. Items may overlap, however no more than 50 percent. On top of the classes, hands may be placed randomly to generate a more realistic representation of what the final system may expect. After all items have been placed, random noise is introduced to the final image. Noise includes black parts and spots over the image to further discourage the network from memorizing rather than learning from the images. Finally, the item position and size are saved. Since we initially planned to use COCO-Format, our annotations were created in this format. This means the annotation for all images from the dataset are saved in a single JSON file. Since we later switched to using a YOLO network, we had to convert the annotations into the new format. YOLO requires a txt file with the class location for every image. An example of a resulting synthetic picture can be seen in the following figure. The shown table displays the quantity of slightly different images available for the synthetic dataset. To test the data generation yourself, follow the DataGeneration.ipynb notebook. Item-class Quantity Fries 19 Hamburger 13 Cheeseburger 5 Big Mac 13 Hamburger Royal TS 15 McRib 7 McChicken Classic 18 Chicken McNuggets 9er 19 McWrap 24 McSundae 26 Softdrink 20 Ketchup 20 Mayonnaise 14 Sauce 28 Hands 17","title":"Data Augmentation"},{"location":"02_data_generation/data_generation/#final-dataset","text":"Our final dataset consists of three different sets. A training dataset used to train the model, a validation dataset used to evaluate the model's performance while training and a test dataset to evaluate the final model performance. The training dataset consists of 45.000 synthetic images and an additional 5000 random images with no classes to further regularize the model. These 5000 images were taken from the COCO2017 dataset. The validation dataset consists of 5000 images, 95% of which are synthetic images, the remaining 5% are randomly selected stock images containing no classes. The test dataset consists of 160 real images that were labeled by hand. These pictures were taken on the system that will be deployed to customers. Distance, camera, lighting, etc. are as specified to gain information about the true real-world performance of the system. The test dataset features 146 highly complex images that may not be found in a real-world application. Classes were stacked on top of each other, were almost completely out-of-frame and include modifications such as straws in the beverages or entirely molten ice cream. This ensures that the performance we evaluate on the test set can be seen as a conservative \"lower bound\". If the system was to be shipped to a customer, we believe to be able to guarantee a performance similar to the evaluation results on the test set. However, in a day to day use a significantly better performance can be expected. Partly because the images should usually not be as complex, and in part, since we classify a continuous stream of images. This means that while a hand may block parts of an image for a couple of frames, by combining the classification results of multiple frames into our final prediction we can get a robust classification that can deal with missing out on some items every couple of frames. We originally planned to create a dataset consisting of a hybrid between synthetic data and real data. Our tests showed that adding real data into the dataset resulted in little to no improvement in model performance, as you can see in the following table. The given metrics are used to describe the accuracy of the model-output over all item-classes. Purely synthetic dataset Hybrid dataset Precision 0.950 0.932 Recall 0.822 0.844 mAP@.5 (threshold at 0.5) 0.893 0.903 mAP@.95 (threshold at 0.95) 0.650 0.641 This small performance gap might be caused by differing quantities in the two sets. While synthetic data is easy and quick to generate, real data needs to be manually annotated which is a slow and tedious process. This resulted in an imbalance between the two groups. While we used 45.000 generated synthetic images, we only added 160 real-world images. The synthetic nature of the training data ensures an unbiased and evenly distributed dataset. Human bias to items positioning, grouping or selection can be almost entirely ruled out. This also shows in the statistical analysis of the training dataset. The items are distributed evenly across the image. All classes have a similar amount of occurrences in the dataset. The width and height of the classes are less evenly distributed. This is the plain result of the shape of the McDonalds packaging and should not influence the models' performance negatively. The test dataset was created by hand. While this means that the distribution among classes is less even than with the synthetic dataset, we still tried to minimize the bias and create an evenly distributed dataset. The class mc_flurry was scratched last minute from the test dataset. This was more logistical than a technical decision since the temperature in Karlsruhe at that time transformed a McFlurry into a McMilkshake within minutes, so difficult decisions had to be made. Apart from the McFlurry, classes are roughly evenly distributed and all classes contain at least a few different samples so the model's performance can be sufficiently evaluated.","title":"Final Dataset"},{"location":"02_data_generation/data_generation/#possible-extensions","text":"In a real-world environment, the synthetic dataset allows a rapid adaption of the model to an ever-changing menu. By adding images of new menu items to the synthetic data generator, new training datasets can be created instantaneously with little to no human input. Especially large QSR chains may benefit from this approach as vector graphics of the menu items and 3D models typically already exist for marketing purposes. By using these images and 3D models the generation of an even more detailed synthetic dataset may be possible. Instead of using simple 2D pixel operations, in real-world 3D renders as training data may further increase model performance by teaching the model about different perspectives and overlaps of items.","title":"Possible Extensions"},{"location":"03_model/model/","text":"Model Computer vision challenges were initially posed by simple geometric body detections - tasks for which neural networks were not necessarily needed. Due to the trend with increasing availability of computing power and the establishment of Convolutional Neural Networks (CNNs), neural networks in general, but especially CNNs in the domain Computer Vision have gained publicity due to very good results. For this reason, we decided to master our object detection using such neural networks - available in the manner of open source in different model architectures. Model Architectures Since the requirements from the course are an object detection in real-time on a Jetson Nano, we had to look for rather small neural network architectures that perform high FPS rates (frames per second) on relatively low processing power. For our use-case, real-time meant to be a smooth detection at at least 8 FPS. The following figure shows different model architectures deployed with NVIDIA's TensorRT accelerator library on a Jetson Nano. After further research, we decided either a model architecture from the Tensorflow Object Detection API or a YOLO architecture implemented with a widespread machine learning library like PyTorch would be the perfect fit, since both are very well documented, and lack of time is one of our biggest challenges. With both libraries as possible solutions, we created training pipelines to test several model architectures, such as the \"SSD Mobilenet-V2\" from the Tensorflow Model Zoo, or a YOLOv4-CSP and evaluate their performance. Finally, we did not compare the performance of different YOLO model architectures with models from the Tensorflow Model-Zoo , because we decided to use Nvidia's DeepStream library , which integrates particularly well with YOLO architectures, for inference on the Jetson Nano. YOLO Architecture YOLO , an abbreviation for the term \"You Only Look Once\", is a model architecture for object detection, employing CNNs to detect in real-time. As the name indicates YOLO architectures are single-stage object detectors. In general, the architecture uses three techniques to achieve accurate results with minimal error and a real-time detection. Residual Blocks: The input-image is divided into grids of a quadratic dimension. Every grid-cell will detect objects that appear within them. Bounding Box Regression: The Bounding Box, as the rectangle outline of an object in an image, consists of height and width, relative x and y center position and the item-class. YOLO uses a single regression to align all Bounding Box factors. Intersection over Union (IOU): IOU is used by the YOLO architecture to provide a bounding box that fits the object perfectly. Each grid-cell is responsible for predicting the bounding box with its confidence score. The above figure shows how the three techniques are applied to produce the final object detection results. YOLO Model Comparison The DeepStream-based inference solution we built for the QSROA supports the deployment of most common YOLO architectures with very little reconfiguration effort. In order to further streamline our development process, we decided to use a model supported by the DeepStream-Yolo project. As a result, we had several architectures from which we could choose the best-suited model according to use case and hardware restrictions. YOLOv5 available in various sizes implements CSP Bottleneck like YOLOv4-CSP YOLOv4x-Mish YOLOv4-CSP (Scaled/ Cross-Stage Partial Networks ) code has some unknown issues with reproducing paper performance outstanding performance CSP architecture reduces inference computation YOLOv4 in general, we can say YOLOv4 is an improvement to YOLOv3 YOLOv4-Tiny reduced network size compared to YOLOv4 Faster prediction but less accuracy YOLOv3-SPP ( Spatial Pyramid Pooling ) useful especially for networks that have many different input image sizes - therefore a not necessary architecture for us YOLOv3 YOLOv3-Tiny-PRN (Partial Residual Networks) has a more stable performance than YOLOv3-Tiny better FPS rate than YOLO-Tiny YOLOv3-Tiny YOLOv3-Lite YOLOv3-Nano YOLO-Fastest YOLO-Fastest-XL YOLOv2 YOLOv2-Tiny Our selection is based on detection performance, speed (FPS) and scalability. By scalability, we refer to the ability to scale a model in size without requiring major changes to the model architecture. After evaluating several model architectures we decided to have a deeper comparison of the YOLOv4-CSP and YOLOv5s . Therefore, we balanced the pros and contra of both models. YOLOv4-CSP Pro: Contra: Research Paper available Low FPS (3.9FPS) or image-stutter High mAP Possible overfitting because of synthetic data Native convert. Code has some unknown issues with reproducing paper performance The following video is a YOLOv4-CSP trained on the COCO2017 Dataset deployed on the Jetson Nano. Without the 5 skip frames that we apply, we achieve a 3.9 FPS-rate in average on the Jetson Nano, which visualized looks like a stutter from frame to frame. YOLOv5s Pro: Contra: High FPS (12FPS) No paper available Potentially less overfitting \"non-transparent\" python code Fast training time Less memory usage Performance reserve because of less memory usage This video shows the YOLOv5s trained on the COCO2017 Dataset, also deployed on the Jetson Nano, without any stutter (1 skip frame; 13 FPS), but as you can see with a slightly worse detection performance than the YOLOv4-CSP. After a long evaluation and weighing up various characteristics we decided YOLOv5s ( Release 5.0 ) , the smallest YOLOv5 model would be the most suitable one for our use case. Especially due to the high scalability and fast training time - both properties that fit very well to the adaptivity of our approach of the synthetic dataset to changing conditions and thus perfectly match the high flexibility we approach to e.g. sessional changes described in the business plan. About YOLOv5(s) YOLOv5 is a GitHub project developed and maintained by the artificial intelligence startup Ultralytics . The project describes itself as a family of object detection architectures and models pretrained on the COCO dataset. While as of now there is no research paper available, it can still be seen as an unofficial successor to YOLOv3 as it incorporates various architectural advancements and new data augmentation techniques for training such as mosaic augmentation. The project is under active development and since the original release in June of 2020, the architecture was updated multiple times to further improve performance. Contrary to the original YOLO, YOLOv5 is implemented using the popular PyTorch framework. YOLOv5 is available in four different sizes. The different sizes with corresponding characteristics are listed in the following table. Model Size (MB) FPS-rate (ms) mAP (COCO) YOLOv5s 14 2.0 37.2 YOLOv5m 41 2.7 44.5 YOLOv5l 90 3.8 48.2 YOLOv5x 168 6.1 50.4 Since the characteristics of the smallest model (YOLOv5s) fit our requirements regarding detection speed and size very well, we will focus more on its technical details in the following (the other listed versions only differ slightly in model layers and number of parameters). The YOLOv5s, due to its single-stage object detection characteristic also consists of three main parts. To extract important features from the input image the model backbone is a Cross Stage Partial network (CSP). For better model generalization the model neck is used to generate feature pyramids like the PANet . The final detection is done in the model head. Bounding Boxes, class probabilities, output vector and the objectness scores are generated on the given features. Training-Pipeline PyTorch YOLOv5s The programmed python script is designed to run on the BWUniCluster because Image-Processing requires high processing power which is often limited on personal computers. With limitations on Google Colab like restricting training time and computing power, we decided to use the Cluster instead. The script used for training can be found under ./model/train/start.sh . It is a simple shell script executing the training scripts found in the repository of the original Yolov5 model. To run the training, place the start.sh script inside a clone of the Yolov5 repository: https://github.com/ultralytics/yolov5 Make sure the --data parameter links to the right dataset. In our case the synthetic dataset we used for training: https://drive.google.com/file/d/116fG7o5RiESG0x1h3kxuD69LcGa73jym/view?usp=sharing Executing the script will train a model from pretrained weights. Since we used a total of 4 GPUs on the cluster, the distributed launch parameter is enabled. Adapt this parameter according to the resources you have available. If your GPU runs out of memory, decrease the batch size to a size that works with your setup. After training, the script converts the model into TensorRT format. This ensures optimal performance when deploying the model to the Jetson Nano. Statistics about your training run can be found in the newly created ./runs/train folder. Results The model was trained for 100 epochs on synthetic data only. For training the 'finetuning' hyperparameters provided by YOLOv5s were used. The model was not trained from scratch but from weights provided by YOLOv5s. These weights were pretrained on the COCO dataset. Overall, the training took just over 8 hours on 4 Nvidia Tesla V100 GPUs. While convergence on the validation data could be observed around the 80 epochs mark, some further hyperparameter tuning might further increase model performance. After 100 epochs the model performs very well on the validation dataset. Keep in mind that the validation dataset consists of only synthetic data as well. Surprisingly despite some heavy regularization the model still achieves very high accuracies. In many cases, the regularization techniques are so intense that recognizing all items in the images is challenging even to humans. The model seems to achieve similar or even performance than a human would on the synthetic dataset. The final performance of the model was evaluated on a test dataset - introduced in the dataset-section never seen before by the model. In contrast to the validation dataset, the test dataset consists of real images only. Keep in mind that the test data contains highly complex data, as explained in the dataset section. The performance figures here therefore should be considered as a lower bound rather than a final model performance. Generally, the model recognized all the classes in the test dataset with high accuracy. As expected, the confusion matrix shows the highest accuracy in larger item-classes with very distinct features, such as the packaging of Big Macs and the McChicken. Problems are found with products that naturally change depending on the environment as well as between products. A good example is the low accuracy of sundaes. Sundaes come in a variety of different dessert sauces, each one stirred slightly different into the ice cream, making every sundae look unique. Additionally, ice cream tends to melt rather quickly, so when capturing images for the test dataset the appearance will change continuously. While this was a challenge for us, it is less critical for real life situations in the restaurant, as the ice is freshly prepared. The high accuracy achieved on recognizing fries shows that these issues can be overcome with more training data. While fries have a high variability within their class, we could create a relatively large amount of training data for them by simply rearranging the fries. This seems to have greatly increased models' understanding. Our hypothesis is that with a sufficiently large amount of training data these issues could also be overcome for other classes. When investigating the models' results, the model seemed to have issues with items being partly out of frame or overlapped by other items. This is especially the case when moving items into the frame or out of the frame. While this issue will decrease the prediction accuracy of the model on our test dataset, in a real world application this has little to no significance. Since the system is observing a continuous stream of images, missing the item for a couple of frames while it is being moved into frame or blocked partially by a hand should not have any consequences. In many cases, the model will be able to recognize the item once it's fully in frame, and the logic we implemented in the backend of our system is able to filter for items that disappear for single frames or are falsely recognized for short time. By averaging over a longer timeframe, we can greatly reduce the models' false recognitions, as illustrated in the following figure.","title":"Model"},{"location":"03_model/model/#model","text":"Computer vision challenges were initially posed by simple geometric body detections - tasks for which neural networks were not necessarily needed. Due to the trend with increasing availability of computing power and the establishment of Convolutional Neural Networks (CNNs), neural networks in general, but especially CNNs in the domain Computer Vision have gained publicity due to very good results. For this reason, we decided to master our object detection using such neural networks - available in the manner of open source in different model architectures.","title":"Model"},{"location":"03_model/model/#model-architectures","text":"Since the requirements from the course are an object detection in real-time on a Jetson Nano, we had to look for rather small neural network architectures that perform high FPS rates (frames per second) on relatively low processing power. For our use-case, real-time meant to be a smooth detection at at least 8 FPS. The following figure shows different model architectures deployed with NVIDIA's TensorRT accelerator library on a Jetson Nano. After further research, we decided either a model architecture from the Tensorflow Object Detection API or a YOLO architecture implemented with a widespread machine learning library like PyTorch would be the perfect fit, since both are very well documented, and lack of time is one of our biggest challenges. With both libraries as possible solutions, we created training pipelines to test several model architectures, such as the \"SSD Mobilenet-V2\" from the Tensorflow Model Zoo, or a YOLOv4-CSP and evaluate their performance. Finally, we did not compare the performance of different YOLO model architectures with models from the Tensorflow Model-Zoo , because we decided to use Nvidia's DeepStream library , which integrates particularly well with YOLO architectures, for inference on the Jetson Nano.","title":"Model Architectures"},{"location":"03_model/model/#yolo-architecture","text":"YOLO , an abbreviation for the term \"You Only Look Once\", is a model architecture for object detection, employing CNNs to detect in real-time. As the name indicates YOLO architectures are single-stage object detectors. In general, the architecture uses three techniques to achieve accurate results with minimal error and a real-time detection. Residual Blocks: The input-image is divided into grids of a quadratic dimension. Every grid-cell will detect objects that appear within them. Bounding Box Regression: The Bounding Box, as the rectangle outline of an object in an image, consists of height and width, relative x and y center position and the item-class. YOLO uses a single regression to align all Bounding Box factors. Intersection over Union (IOU): IOU is used by the YOLO architecture to provide a bounding box that fits the object perfectly. Each grid-cell is responsible for predicting the bounding box with its confidence score. The above figure shows how the three techniques are applied to produce the final object detection results.","title":"YOLO Architecture"},{"location":"03_model/model/#yolo-model-comparison","text":"The DeepStream-based inference solution we built for the QSROA supports the deployment of most common YOLO architectures with very little reconfiguration effort. In order to further streamline our development process, we decided to use a model supported by the DeepStream-Yolo project. As a result, we had several architectures from which we could choose the best-suited model according to use case and hardware restrictions. YOLOv5 available in various sizes implements CSP Bottleneck like YOLOv4-CSP YOLOv4x-Mish YOLOv4-CSP (Scaled/ Cross-Stage Partial Networks ) code has some unknown issues with reproducing paper performance outstanding performance CSP architecture reduces inference computation YOLOv4 in general, we can say YOLOv4 is an improvement to YOLOv3 YOLOv4-Tiny reduced network size compared to YOLOv4 Faster prediction but less accuracy YOLOv3-SPP ( Spatial Pyramid Pooling ) useful especially for networks that have many different input image sizes - therefore a not necessary architecture for us YOLOv3 YOLOv3-Tiny-PRN (Partial Residual Networks) has a more stable performance than YOLOv3-Tiny better FPS rate than YOLO-Tiny YOLOv3-Tiny YOLOv3-Lite YOLOv3-Nano YOLO-Fastest YOLO-Fastest-XL YOLOv2 YOLOv2-Tiny Our selection is based on detection performance, speed (FPS) and scalability. By scalability, we refer to the ability to scale a model in size without requiring major changes to the model architecture. After evaluating several model architectures we decided to have a deeper comparison of the YOLOv4-CSP and YOLOv5s . Therefore, we balanced the pros and contra of both models. YOLOv4-CSP Pro: Contra: Research Paper available Low FPS (3.9FPS) or image-stutter High mAP Possible overfitting because of synthetic data Native convert. Code has some unknown issues with reproducing paper performance The following video is a YOLOv4-CSP trained on the COCO2017 Dataset deployed on the Jetson Nano. Without the 5 skip frames that we apply, we achieve a 3.9 FPS-rate in average on the Jetson Nano, which visualized looks like a stutter from frame to frame. YOLOv5s Pro: Contra: High FPS (12FPS) No paper available Potentially less overfitting \"non-transparent\" python code Fast training time Less memory usage Performance reserve because of less memory usage This video shows the YOLOv5s trained on the COCO2017 Dataset, also deployed on the Jetson Nano, without any stutter (1 skip frame; 13 FPS), but as you can see with a slightly worse detection performance than the YOLOv4-CSP. After a long evaluation and weighing up various characteristics we decided YOLOv5s ( Release 5.0 ) , the smallest YOLOv5 model would be the most suitable one for our use case. Especially due to the high scalability and fast training time - both properties that fit very well to the adaptivity of our approach of the synthetic dataset to changing conditions and thus perfectly match the high flexibility we approach to e.g. sessional changes described in the business plan.","title":"YOLO Model Comparison"},{"location":"03_model/model/#about-yolov5s","text":"YOLOv5 is a GitHub project developed and maintained by the artificial intelligence startup Ultralytics . The project describes itself as a family of object detection architectures and models pretrained on the COCO dataset. While as of now there is no research paper available, it can still be seen as an unofficial successor to YOLOv3 as it incorporates various architectural advancements and new data augmentation techniques for training such as mosaic augmentation. The project is under active development and since the original release in June of 2020, the architecture was updated multiple times to further improve performance. Contrary to the original YOLO, YOLOv5 is implemented using the popular PyTorch framework. YOLOv5 is available in four different sizes. The different sizes with corresponding characteristics are listed in the following table. Model Size (MB) FPS-rate (ms) mAP (COCO) YOLOv5s 14 2.0 37.2 YOLOv5m 41 2.7 44.5 YOLOv5l 90 3.8 48.2 YOLOv5x 168 6.1 50.4 Since the characteristics of the smallest model (YOLOv5s) fit our requirements regarding detection speed and size very well, we will focus more on its technical details in the following (the other listed versions only differ slightly in model layers and number of parameters). The YOLOv5s, due to its single-stage object detection characteristic also consists of three main parts. To extract important features from the input image the model backbone is a Cross Stage Partial network (CSP). For better model generalization the model neck is used to generate feature pyramids like the PANet . The final detection is done in the model head. Bounding Boxes, class probabilities, output vector and the objectness scores are generated on the given features.","title":"About YOLOv5(s)"},{"location":"03_model/model/#training-pipeline-pytorch-yolov5s","text":"The programmed python script is designed to run on the BWUniCluster because Image-Processing requires high processing power which is often limited on personal computers. With limitations on Google Colab like restricting training time and computing power, we decided to use the Cluster instead. The script used for training can be found under ./model/train/start.sh . It is a simple shell script executing the training scripts found in the repository of the original Yolov5 model. To run the training, place the start.sh script inside a clone of the Yolov5 repository: https://github.com/ultralytics/yolov5 Make sure the --data parameter links to the right dataset. In our case the synthetic dataset we used for training: https://drive.google.com/file/d/116fG7o5RiESG0x1h3kxuD69LcGa73jym/view?usp=sharing Executing the script will train a model from pretrained weights. Since we used a total of 4 GPUs on the cluster, the distributed launch parameter is enabled. Adapt this parameter according to the resources you have available. If your GPU runs out of memory, decrease the batch size to a size that works with your setup. After training, the script converts the model into TensorRT format. This ensures optimal performance when deploying the model to the Jetson Nano. Statistics about your training run can be found in the newly created ./runs/train folder.","title":"Training-Pipeline PyTorch YOLOv5s"},{"location":"03_model/model/#results","text":"The model was trained for 100 epochs on synthetic data only. For training the 'finetuning' hyperparameters provided by YOLOv5s were used. The model was not trained from scratch but from weights provided by YOLOv5s. These weights were pretrained on the COCO dataset. Overall, the training took just over 8 hours on 4 Nvidia Tesla V100 GPUs. While convergence on the validation data could be observed around the 80 epochs mark, some further hyperparameter tuning might further increase model performance. After 100 epochs the model performs very well on the validation dataset. Keep in mind that the validation dataset consists of only synthetic data as well. Surprisingly despite some heavy regularization the model still achieves very high accuracies. In many cases, the regularization techniques are so intense that recognizing all items in the images is challenging even to humans. The model seems to achieve similar or even performance than a human would on the synthetic dataset. The final performance of the model was evaluated on a test dataset - introduced in the dataset-section never seen before by the model. In contrast to the validation dataset, the test dataset consists of real images only. Keep in mind that the test data contains highly complex data, as explained in the dataset section. The performance figures here therefore should be considered as a lower bound rather than a final model performance. Generally, the model recognized all the classes in the test dataset with high accuracy. As expected, the confusion matrix shows the highest accuracy in larger item-classes with very distinct features, such as the packaging of Big Macs and the McChicken. Problems are found with products that naturally change depending on the environment as well as between products. A good example is the low accuracy of sundaes. Sundaes come in a variety of different dessert sauces, each one stirred slightly different into the ice cream, making every sundae look unique. Additionally, ice cream tends to melt rather quickly, so when capturing images for the test dataset the appearance will change continuously. While this was a challenge for us, it is less critical for real life situations in the restaurant, as the ice is freshly prepared. The high accuracy achieved on recognizing fries shows that these issues can be overcome with more training data. While fries have a high variability within their class, we could create a relatively large amount of training data for them by simply rearranging the fries. This seems to have greatly increased models' understanding. Our hypothesis is that with a sufficiently large amount of training data these issues could also be overcome for other classes. When investigating the models' results, the model seemed to have issues with items being partly out of frame or overlapped by other items. This is especially the case when moving items into the frame or out of the frame. While this issue will decrease the prediction accuracy of the model on our test dataset, in a real world application this has little to no significance. Since the system is observing a continuous stream of images, missing the item for a couple of frames while it is being moved into frame or blocked partially by a hand should not have any consequences. In many cases, the model will be able to recognize the item once it's fully in frame, and the logic we implemented in the backend of our system is able to filter for items that disappear for single frames or are falsely recognized for short time. By averaging over a longer timeframe, we can greatly reduce the models' false recognitions, as illustrated in the following figure.","title":"Results"},{"location":"04_backend_frontend/system_architecture/","text":"System Architecture The QSROA system consists of three independent components: the inference pipeline, a backend and a frontend. The inference process takes care of the deployment of our model and provides an RTSP video stream. The backend is responsible for the logical analysis of the detected objects and the frontend is used to display the results and furthermore demonstrates a use case. In the following figure the parts of our systems are illustrated. The frontend is developed by using React, Material-UI and nginx. It communicates with the backend via Socket.IO and REST Endpoints. The backend is developed with Flask, Socket.IO and gunicorn. The inference pipeline is based on Nvidias DeepStream SDK for Python and sends data on detected objects to the backend via a Socket.IO connection. All three components are deployed using Docker. The components are described in detail in the following paragraphs. Inference Due to the confined availability of resources, hardware limitations can pose a significant challenge for the deployment of computer vision models on portable and low-power devices. Therefore, the characteristics of target platforms must be carefully considered when designing a model. However, driven by the increasing demand for on-device AI and mobile gaming, powerful mobile SOCs, offering both, high graphics performance and specialized hardware for machine learning tasks, have become widely available. Nevertheless, utilizing the full potential of these platforms often involves the use of proprietary frameworks and is consequently still a cumbersome process. While the performance of the Nvidia Jetson Nano platform, as the target platform for this project, is easily outperformed by more recent devices such as modern smartphones, it still offers access to a rich ecosystem of GPU-accelerated frameworks provided by Nvidia. Most common machine learning frameworks, including TensorFlow and PyTorch, offer support for GPU-accelerated inference backed by Nvidia CUDA . However, Nvidia TensorRT is the preferred inference framework for the deployment of deep learning models on Jetson devices. It maximizes throughput and reduces latency by automatically applying various optimization techniques. While the inference step is most resource-demanding, creating a real-time computer vision application involves numerous other processing steps. First, the image must be preprocessed and transformed to the format required by the inference engine. Following this step, the model output has to be processed (e.g. performing Non-maximum Suppression, drawing bounding boxes). Finally, depending on the required output format, the video stream needs to be encoded. Specialized and highly performant frameworks such as OpenCV for video processing are available for all these steps. A basic approach to creating a processing pipeline would involve sequentially passing data between these steps in a loop. While this can deliver decent results, it creates plenty of overhead. Most noticeably, data must be copied between GPU and CPU memory multiple times. There are several approaches for addressing those shortcomings. After extensive investigation, we figured that Nvidia's DeepStream SDK best matches the requirements for the QSROA project. As a framework specifically targeted towards the development of AI-powered intelligent video analytics apps and services, it offers a high level of flexibility and customizability. Resulting from a tight integration into the Jetson ecosystem, it also offers superior performance to other solutions. DeepStream is based on the popular pipeline-driven media framework GStreamer and extends upon it by implementing a range of additional plugins specifically targeted towards AI applications. These plugins are optimized for Jetson devices and Nvidia Datacenter GPUs. As such it allows for the integration of TensorRT inference into a GStreamer media pipeline. Technical Details The QSROA inference solution is implemented using the DeepStream SDK for Python and consists of two major components. The DeepStream pipeline itself and a Python script responsible for managing the pipeline process and communication with the backend. The DeepStream pipeline takes the input image provided by the CSI-Camera attached to the Jetson Nano. The generated GStreamer buffers are passed through the pipeline, applying different preprocessing steps such as downsizing the input image. Data is then passed to the Gst-nvinfer component responsible for TensorRT inference. Further details on inference are explained in section TensorRT Inference . As part of inference metadata containing information on recognized objects and more are attached to the buffers. After inference buffers are (optionally) passed to the Gst-nvtracker component executing unique object tracking. The output image is then processed further (e.g. bounding boxes are drawn) and is either output as a windowed video stream on the Jetson device or as a low-latency RTSP video stream. As RTSP is a common streaming protocol for IP cameras and CCTV surveillance, it is most likely already in use at the majority of POS locations. QSROA customers may therefore choose to integrate this stream into their existing systems. TensorRT Inference While Nvidia provides samples for the deployment of YOLO models using TensorRT and DeepStream, newer YOLO models such as the YOLOv5 model used by the QSROA are not yet supported. However, the provided source can be updated to support these models. As such, there is an official TensorRT implementation of YOLOv5 that can be used to obtain a TensorRT engine file upon compilation. This engine can then be used along with the necessary nvinfer plugin provided by the DeepStream-Yolo project on GitHub. Updates were applied to both projects' source code to match QSROA requirements. Object Tracking and Skip-Inference The Gst-nvtracker plugin can be used to track detected objects in DeepStream pipelines. This allows assigning unique ids to objects and tracking their movement over time. The DeepStream SDK provides three different low-level tracker implementations that can be chosen depending on project requirements. Unique object tracking could therefore be used to implement more advanced features. However, this functionality is not yet used for the QSROA. As object trackers typically consume noticeably fewer resources than TensorRT inference, they can also be used to optimize perceived video stream performance. The QSROA DeepStream pipeline achieves this by skipping inference for every 2^nd^ frame. As objects are tracked by the tracker component, bounding boxes can still be updated every frame, resulting in almost doubling the framerate while still being able to keep track of detected objects. Applying skip-inference is vastly beneficial for model selection as it allows for the deployment of models that would otherwise be too large for delivering an acceptable framerate. This even allows for the deployment of a YOLOv4-CSP or full-size YOLOv4 model on a Jetson Nano as demonstrated in the section model. Statistics Collection To supply the backend with necessary information it is required to extract the metadata from the stream. For DeepStream pipelines, it is common practice to insert a (blocking) callback before the Gst-nvdsosd component, which is amongst other things responsible for drawing bounding boxes. The callback receives the buffers that would otherwise be directly passed to the nvdsosd component. This allows for applying changes to the metadata or adding additional text to be drawn. As by that time the buffers already contain all the necessary metadata, it can also be used to collect the information required by the backend. We do this by counting all objects in a frame (grouped by class). The information is then added to a global two-dimensional dictionary. The first index of this dictionary corresponds to the label of the object class whereas the second object corresponds to how often this class appears in the frame. The integer value is incremented by one for each key-pair obtained from counting objects in the frame. Every second this dictionary is submitted to a multiprocessing queue along with a timestamp and the total number of frames since last submitting to the queue. The dictionary is afterward cleared. The Python script responsible for managing the inference process collects the information from the multiprocessing queue and acts as a Socket.IO client that sends the data to the backend for further processing. The following example shows the format of the dictionary sent to the backend. As can be obtained from the information contained in the dictionary, two objects of class beverage were detected in 2 of the last 20 frames while three objects of class beverage were detected in 18 frames. An object of type fries was detected in all 20 frames. { 'frames_since_last_report': 20, 'objects': { 'beverage': { '2': 2, '3': 18 }, 'fries': {'1': 20} }, 'timestamp': 1625162677116201 } Backend In the case of the QSROA, the backend has the central task of bringing together the results produced by the model with the logic and functionality necessary in order to deliver the advanced service. Its structure and way of functioning should therefore be described below. Structure The backend has been created with Flask. It is constructed through python Socket.IO and REST endpoints which may be described in the following: /send-orders : expects a JSON object in the format of { 'item': number_of_items } example: { 'cheeseburger': 1, 'hamburger': 3, 'chickenburger': 2, } /delete-orders : delete all orders Additionally, the inference process, described in the previous paragraph, subscribes to the Socket.IO connection statistics . The inference client sends a JSON object containing statistical data at a frequency of 1 Hz. The frontend subscribes to the Socket.IO connections actual-order and orders . Functionality The functionality of the backend crucially involves a parameter named FRAMES . The parameter describes the percentage of frames in which an object must be detected by the model in order to be interpreted as an actual object. It therefore can take on values in the range of [0, 1] and functions as a noise filter. In the scope of the lecture this value has arbitrarily been set to 0.8. However, going forward it can certainly be optimized empirically. FRAMES is then subsequently used to calculate another variable min_frames , with min_frames = FRAMES * frames_since_last_report . With this min_frames calculates the product of FRAMES and the number of frames captured since the last statistical analysis ( frames_since_last_report ). It therefore acts as the lower bound for the number of frames an object must be detected in to actually be considered an object by the QSROA. With min_frames being determined, for every item class in the statistical data provided, the following calculation is being conducted: For every item class the number of items gets identified. This happens by identifying the maximum number of instances of an item class for which the min_frames requirement is being fulfilled. For clarification consider the following example: min_frames has been calculated as twelve. The model detected exactly four beverages in 3 frames, exactly three beverages in 11 frames and exactly two beverages in 2 frames. Obviously, if the model detects four beverages it implicitly detected two and three beverages as well. As four beverages are detected in less frames than min_frames it gets discarded. Both three and two beverages got identified in more than 12 frames (three in 3+11 and two in 3+11+2). The three beverages therefore equal the maximum item of numbers, so that the statistical calculation returns the result number of beverages is three. Thus, the calculation returns a list containing class items and their respective quantity. In a final step the calculated list is then compared with the current order, resulting in three lists. correct: Products that have been detected as well as found in the order missing: Products that have been found in the order but not detected incorrect: Products that have been detected but not found in the order The three lists as well as the next eight elements of the order queue are then sent to the frontend via the Socket.IO connections. Notes The systems processes orders by the FIFO principal. In order to be able to develop and test the backend independently from the jetson or rather the inference process, the backend has been extended by the file mock.py . With execution a Socket.IO connection to the backend is being established. Via the connection a JSON object containing the statistical data is being sent to the backend either in predefined temporal intervals or by operating the enter-button. For further information on the deployment, we refer to the chapter deployment. More information on how to run the backend as well as some hints can be found in the README of the backend directory. Frontend As we intend to demonstrate the QSROA and its functionalities in a best possible and most realistic scenario, our project includes two frontends for distinct application areas. The first frontend focuses on our customer's customers by providing a checkout system. The second view displays the central features of our service, serving as an order control screen to our customer's employees. For reasons of simplicity both frontends have been developed within the same project. The Checkout System The checkout system functions as a classic checkout system. It enables the restaurant's customers to put in their orders. The orders may be composed of an arbitrary combination of the items shown in the UI. It has been designed so that the input can take place via tablets and is touch-capable, therefore resembling current implementations of QSR chains. To submit an order, the send button must be pressed. To delete the current selection, one must press the delete button. In a field project the checkout system would be replaced by our customers checkout system. The Order Control Screen The order control screen is the central visual element of the QSROA as it brings together the functionalities of the neural network and backend with an UI that enables the employees to profit from the added value. It is the view that our customer's employees are intended to work with. The order control screen can be separated into two parts. The first part displays the current order, meaning the current order the employee is supposed to process. The UI supports the employee by differentiating between items that have been added correctly so (complete) and those that are still missing (missing). Additionally, it also warns the employee of items that have been added mistakenly to the order (incorrect). To further support the employee the three categories get clarified by the use of a traffic light scheme. The second part of the screen shows the subsequently following orders, so that the restaurant can act participatory and optimize its processes. Development Process The frontend has been developed with React. For the creation of the app create-react-app was used. For the design the Material-UI package was added to the project. As a client for REST axios was chosen, for WebSocket connections the interfaces make use of Socket.IO. The source of pictures utilized in the checkout system UI has been McDonald's ( McDonald's ). The structure of the UI is component based. Exemplary components are order lists and the traffic light list.","title":"System Architecture"},{"location":"04_backend_frontend/system_architecture/#system-architecture","text":"The QSROA system consists of three independent components: the inference pipeline, a backend and a frontend. The inference process takes care of the deployment of our model and provides an RTSP video stream. The backend is responsible for the logical analysis of the detected objects and the frontend is used to display the results and furthermore demonstrates a use case. In the following figure the parts of our systems are illustrated. The frontend is developed by using React, Material-UI and nginx. It communicates with the backend via Socket.IO and REST Endpoints. The backend is developed with Flask, Socket.IO and gunicorn. The inference pipeline is based on Nvidias DeepStream SDK for Python and sends data on detected objects to the backend via a Socket.IO connection. All three components are deployed using Docker. The components are described in detail in the following paragraphs.","title":"System Architecture"},{"location":"04_backend_frontend/system_architecture/#inference","text":"Due to the confined availability of resources, hardware limitations can pose a significant challenge for the deployment of computer vision models on portable and low-power devices. Therefore, the characteristics of target platforms must be carefully considered when designing a model. However, driven by the increasing demand for on-device AI and mobile gaming, powerful mobile SOCs, offering both, high graphics performance and specialized hardware for machine learning tasks, have become widely available. Nevertheless, utilizing the full potential of these platforms often involves the use of proprietary frameworks and is consequently still a cumbersome process. While the performance of the Nvidia Jetson Nano platform, as the target platform for this project, is easily outperformed by more recent devices such as modern smartphones, it still offers access to a rich ecosystem of GPU-accelerated frameworks provided by Nvidia. Most common machine learning frameworks, including TensorFlow and PyTorch, offer support for GPU-accelerated inference backed by Nvidia CUDA . However, Nvidia TensorRT is the preferred inference framework for the deployment of deep learning models on Jetson devices. It maximizes throughput and reduces latency by automatically applying various optimization techniques. While the inference step is most resource-demanding, creating a real-time computer vision application involves numerous other processing steps. First, the image must be preprocessed and transformed to the format required by the inference engine. Following this step, the model output has to be processed (e.g. performing Non-maximum Suppression, drawing bounding boxes). Finally, depending on the required output format, the video stream needs to be encoded. Specialized and highly performant frameworks such as OpenCV for video processing are available for all these steps. A basic approach to creating a processing pipeline would involve sequentially passing data between these steps in a loop. While this can deliver decent results, it creates plenty of overhead. Most noticeably, data must be copied between GPU and CPU memory multiple times. There are several approaches for addressing those shortcomings. After extensive investigation, we figured that Nvidia's DeepStream SDK best matches the requirements for the QSROA project. As a framework specifically targeted towards the development of AI-powered intelligent video analytics apps and services, it offers a high level of flexibility and customizability. Resulting from a tight integration into the Jetson ecosystem, it also offers superior performance to other solutions. DeepStream is based on the popular pipeline-driven media framework GStreamer and extends upon it by implementing a range of additional plugins specifically targeted towards AI applications. These plugins are optimized for Jetson devices and Nvidia Datacenter GPUs. As such it allows for the integration of TensorRT inference into a GStreamer media pipeline.","title":"Inference"},{"location":"04_backend_frontend/system_architecture/#technical-details","text":"The QSROA inference solution is implemented using the DeepStream SDK for Python and consists of two major components. The DeepStream pipeline itself and a Python script responsible for managing the pipeline process and communication with the backend. The DeepStream pipeline takes the input image provided by the CSI-Camera attached to the Jetson Nano. The generated GStreamer buffers are passed through the pipeline, applying different preprocessing steps such as downsizing the input image. Data is then passed to the Gst-nvinfer component responsible for TensorRT inference. Further details on inference are explained in section TensorRT Inference . As part of inference metadata containing information on recognized objects and more are attached to the buffers. After inference buffers are (optionally) passed to the Gst-nvtracker component executing unique object tracking. The output image is then processed further (e.g. bounding boxes are drawn) and is either output as a windowed video stream on the Jetson device or as a low-latency RTSP video stream. As RTSP is a common streaming protocol for IP cameras and CCTV surveillance, it is most likely already in use at the majority of POS locations. QSROA customers may therefore choose to integrate this stream into their existing systems.","title":"Technical Details"},{"location":"04_backend_frontend/system_architecture/#tensorrt-inference","text":"While Nvidia provides samples for the deployment of YOLO models using TensorRT and DeepStream, newer YOLO models such as the YOLOv5 model used by the QSROA are not yet supported. However, the provided source can be updated to support these models. As such, there is an official TensorRT implementation of YOLOv5 that can be used to obtain a TensorRT engine file upon compilation. This engine can then be used along with the necessary nvinfer plugin provided by the DeepStream-Yolo project on GitHub. Updates were applied to both projects' source code to match QSROA requirements.","title":"TensorRT Inference"},{"location":"04_backend_frontend/system_architecture/#object-tracking-and-skip-inference","text":"The Gst-nvtracker plugin can be used to track detected objects in DeepStream pipelines. This allows assigning unique ids to objects and tracking their movement over time. The DeepStream SDK provides three different low-level tracker implementations that can be chosen depending on project requirements. Unique object tracking could therefore be used to implement more advanced features. However, this functionality is not yet used for the QSROA. As object trackers typically consume noticeably fewer resources than TensorRT inference, they can also be used to optimize perceived video stream performance. The QSROA DeepStream pipeline achieves this by skipping inference for every 2^nd^ frame. As objects are tracked by the tracker component, bounding boxes can still be updated every frame, resulting in almost doubling the framerate while still being able to keep track of detected objects. Applying skip-inference is vastly beneficial for model selection as it allows for the deployment of models that would otherwise be too large for delivering an acceptable framerate. This even allows for the deployment of a YOLOv4-CSP or full-size YOLOv4 model on a Jetson Nano as demonstrated in the section model.","title":"Object Tracking and Skip-Inference"},{"location":"04_backend_frontend/system_architecture/#statistics-collection","text":"To supply the backend with necessary information it is required to extract the metadata from the stream. For DeepStream pipelines, it is common practice to insert a (blocking) callback before the Gst-nvdsosd component, which is amongst other things responsible for drawing bounding boxes. The callback receives the buffers that would otherwise be directly passed to the nvdsosd component. This allows for applying changes to the metadata or adding additional text to be drawn. As by that time the buffers already contain all the necessary metadata, it can also be used to collect the information required by the backend. We do this by counting all objects in a frame (grouped by class). The information is then added to a global two-dimensional dictionary. The first index of this dictionary corresponds to the label of the object class whereas the second object corresponds to how often this class appears in the frame. The integer value is incremented by one for each key-pair obtained from counting objects in the frame. Every second this dictionary is submitted to a multiprocessing queue along with a timestamp and the total number of frames since last submitting to the queue. The dictionary is afterward cleared. The Python script responsible for managing the inference process collects the information from the multiprocessing queue and acts as a Socket.IO client that sends the data to the backend for further processing. The following example shows the format of the dictionary sent to the backend. As can be obtained from the information contained in the dictionary, two objects of class beverage were detected in 2 of the last 20 frames while three objects of class beverage were detected in 18 frames. An object of type fries was detected in all 20 frames. { 'frames_since_last_report': 20, 'objects': { 'beverage': { '2': 2, '3': 18 }, 'fries': {'1': 20} }, 'timestamp': 1625162677116201 }","title":"Statistics Collection"},{"location":"04_backend_frontend/system_architecture/#backend","text":"In the case of the QSROA, the backend has the central task of bringing together the results produced by the model with the logic and functionality necessary in order to deliver the advanced service. Its structure and way of functioning should therefore be described below.","title":"Backend"},{"location":"04_backend_frontend/system_architecture/#structure","text":"The backend has been created with Flask. It is constructed through python Socket.IO and REST endpoints which may be described in the following: /send-orders : expects a JSON object in the format of { 'item': number_of_items } example: { 'cheeseburger': 1, 'hamburger': 3, 'chickenburger': 2, } /delete-orders : delete all orders Additionally, the inference process, described in the previous paragraph, subscribes to the Socket.IO connection statistics . The inference client sends a JSON object containing statistical data at a frequency of 1 Hz. The frontend subscribes to the Socket.IO connections actual-order and orders .","title":"Structure"},{"location":"04_backend_frontend/system_architecture/#functionality","text":"The functionality of the backend crucially involves a parameter named FRAMES . The parameter describes the percentage of frames in which an object must be detected by the model in order to be interpreted as an actual object. It therefore can take on values in the range of [0, 1] and functions as a noise filter. In the scope of the lecture this value has arbitrarily been set to 0.8. However, going forward it can certainly be optimized empirically. FRAMES is then subsequently used to calculate another variable min_frames , with min_frames = FRAMES * frames_since_last_report . With this min_frames calculates the product of FRAMES and the number of frames captured since the last statistical analysis ( frames_since_last_report ). It therefore acts as the lower bound for the number of frames an object must be detected in to actually be considered an object by the QSROA. With min_frames being determined, for every item class in the statistical data provided, the following calculation is being conducted: For every item class the number of items gets identified. This happens by identifying the maximum number of instances of an item class for which the min_frames requirement is being fulfilled. For clarification consider the following example: min_frames has been calculated as twelve. The model detected exactly four beverages in 3 frames, exactly three beverages in 11 frames and exactly two beverages in 2 frames. Obviously, if the model detects four beverages it implicitly detected two and three beverages as well. As four beverages are detected in less frames than min_frames it gets discarded. Both three and two beverages got identified in more than 12 frames (three in 3+11 and two in 3+11+2). The three beverages therefore equal the maximum item of numbers, so that the statistical calculation returns the result number of beverages is three. Thus, the calculation returns a list containing class items and their respective quantity. In a final step the calculated list is then compared with the current order, resulting in three lists. correct: Products that have been detected as well as found in the order missing: Products that have been found in the order but not detected incorrect: Products that have been detected but not found in the order The three lists as well as the next eight elements of the order queue are then sent to the frontend via the Socket.IO connections.","title":"Functionality"},{"location":"04_backend_frontend/system_architecture/#notes","text":"The systems processes orders by the FIFO principal. In order to be able to develop and test the backend independently from the jetson or rather the inference process, the backend has been extended by the file mock.py . With execution a Socket.IO connection to the backend is being established. Via the connection a JSON object containing the statistical data is being sent to the backend either in predefined temporal intervals or by operating the enter-button. For further information on the deployment, we refer to the chapter deployment. More information on how to run the backend as well as some hints can be found in the README of the backend directory.","title":"Notes"},{"location":"04_backend_frontend/system_architecture/#frontend","text":"As we intend to demonstrate the QSROA and its functionalities in a best possible and most realistic scenario, our project includes two frontends for distinct application areas. The first frontend focuses on our customer's customers by providing a checkout system. The second view displays the central features of our service, serving as an order control screen to our customer's employees. For reasons of simplicity both frontends have been developed within the same project.","title":"Frontend"},{"location":"04_backend_frontend/system_architecture/#the-checkout-system","text":"The checkout system functions as a classic checkout system. It enables the restaurant's customers to put in their orders. The orders may be composed of an arbitrary combination of the items shown in the UI. It has been designed so that the input can take place via tablets and is touch-capable, therefore resembling current implementations of QSR chains. To submit an order, the send button must be pressed. To delete the current selection, one must press the delete button. In a field project the checkout system would be replaced by our customers checkout system.","title":"The Checkout System"},{"location":"04_backend_frontend/system_architecture/#the-order-control-screen","text":"The order control screen is the central visual element of the QSROA as it brings together the functionalities of the neural network and backend with an UI that enables the employees to profit from the added value. It is the view that our customer's employees are intended to work with. The order control screen can be separated into two parts. The first part displays the current order, meaning the current order the employee is supposed to process. The UI supports the employee by differentiating between items that have been added correctly so (complete) and those that are still missing (missing). Additionally, it also warns the employee of items that have been added mistakenly to the order (incorrect). To further support the employee the three categories get clarified by the use of a traffic light scheme. The second part of the screen shows the subsequently following orders, so that the restaurant can act participatory and optimize its processes.","title":"The Order Control Screen"},{"location":"04_backend_frontend/system_architecture/#development-process","text":"The frontend has been developed with React. For the creation of the app create-react-app was used. For the design the Material-UI package was added to the project. As a client for REST axios was chosen, for WebSocket connections the interfaces make use of Socket.IO. The source of pictures utilized in the checkout system UI has been McDonald's ( McDonald's ). The structure of the UI is component based. Exemplary components are order lists and the traffic light list.","title":"Development Process"},{"location":"05_deployment/deployment/","text":"Deployment One of our main objectives of this class and its group work was to deliver a service that was actually value-adding to a real-life problem. Our solution had to be as practical and realistic as possible. Thus, it was clear early on, that the deployment would play a crucial role in achieving this goal. We did, however, manage to deliver our service in a form deployable on a single Jetson Nano including backend as well as frontend. Hence, we are able to present a solution that is easily deployable on a single small and cost-effective device. This chapter elaborates on how the deployment process of the QSROA works and how the device can be optimally set-up for use. Hardware Setup With packing stations of QSRs being the main area of application of the QSROA, we suggest a somewhat standardized setup of the hardware. The setup essentially revolves around the camera. We suggest that the camera and therefore the Jetson Nano is mounted at a fixed height of one meter above the counter. Further, we advise to place the camera in parallel to the surface of the counter. The Jetson itself needs to be connected to a local network via LAN. The packing area should preferably be well lit to avoid extensive shadows. There are no recommendations regarding the surface structure or the color of the counter. Docker Originally, we planned to do the complete developing process on other computers to subsequently build the Docker containers and upload them to a container registry. On the Jetson Nano one would then only call docker-compose to pull and start the containers. However, this concept led us into some problems as certain steps of our build process require the DeepStream and CUDA SDK to be present on the build system. The latter of which is only compatible with certain Nvidia GPUs. It may have been possible to build an adequate virtual machine to overcome the issue, anyhow the effort required for this would not have been justifiable. We therefore went with an adapted concept. This concept includes three distinct Docker Containers: one for the frontend, one for the backend and one for the inference. Now as before, both frontend and backend are built on PCs. By contrast, the inference container is being built on the Jetson Nano. In the following the containers are depicted in more detail. The creation of the container for the fronted starts with the conduction of a production build of the React application in a node.js container. Therefore, the needed packages ( package.json ) are installed and the source code of the application is copied into the container. The resulting production build is then copied into a nginx container, which we use to host the react website. nginx is a webserver software. For the backend, the source-files as well as the requirements.txt are copied into a python container. Then, the required packages are installed with pip. In a final step, the backend is executed via gunicorn, a http-server provider for python. Nvidia actually supplies a Docker base image for DeepStream , which is used for the inference. All necessary libraries are installed in the Docker Container and gst-python is being pulled and built from the GStreamer Github repo. Next, the files from the inference folder are copied into the container, the requirements.txt installed and run_server.py executed. run_server.py executes the deepstream pipeline and takes care of the Socket.IO connection to the backend. GitLab Pipeline Originally it was planned to use a GitLab Runner for automated Docker builds. Merge requests in the master branch would trigger the pipeline to automatically build a new Docker container and upload it to the container registry. While the pipeline worked well for the frontend, the pushing already caused trouble. As the inference container must be built with scripts on the Jetson Nano either way, we decided to take another path. Thus, we ended up outright working with build scripts. Of the two alternatives, it came out as significantly less effortful. The fact that a limited number of people worked on this project in a brief time span supports this decision. Build Scripts As elaborated in the preceding paragraphs, both frontend and backend Docker containers are built on PCs. Unfortunately, KIT GitLab does not provide a container registry, so that we had to extend the project by a GitHub account, named \"aissgroup\". In the respective folders of frontend and backend, a script is available named deploy-arm.sh . These scripts build Docker Containers for the Jetson and push them to the container registry. For testing purposes on Windows, there are additional scripts available named deploy.sh . They differ in a way that the Docker Containers are now created for Windows. Unlike front- and backend, the inference must be built with scripts directly on the Jetson. Thus, the script named build.sh is to be executed on the device. The script creates a temporary build folder, in which all relevant files such as Github projects, model weights and labels are copied. The first thing built is the yolov5s.engine . Subsequently a libnvdsinfer_custom_impl_Yolo.so Library is built by the script. Finally, both files and labels.txt are copied into a folder named output. To finally deploy the whole service on the Jetson, two more scripts need to be used. They can be found on the top level of the project folder and are named build_all.sh and start_all.sh . Both of them must be executed on the Jetson. build_all.sh deletes all existing Docker Containers and images to be able to conduct a clean, new build and avoid accidental uses of old images. Following, both the frontend and the backend container get pulled from the repository and the inference Docker Container is being built. After successfully running the build script, the start_all.sh script is to be executed. The script stops all running containers and deletes them. It then starts the three containers of the QSROA application and exposes the following ports in the network: 5001 Backend Port 1234 Frontend Port 8554 RTSP Port It was decided to go with two separate scripts for the build and the execution as it enables repetitive starts of the application without rebuilding.","title":"Deployment"},{"location":"05_deployment/deployment/#deployment","text":"One of our main objectives of this class and its group work was to deliver a service that was actually value-adding to a real-life problem. Our solution had to be as practical and realistic as possible. Thus, it was clear early on, that the deployment would play a crucial role in achieving this goal. We did, however, manage to deliver our service in a form deployable on a single Jetson Nano including backend as well as frontend. Hence, we are able to present a solution that is easily deployable on a single small and cost-effective device. This chapter elaborates on how the deployment process of the QSROA works and how the device can be optimally set-up for use.","title":"Deployment"},{"location":"05_deployment/deployment/#hardware-setup","text":"With packing stations of QSRs being the main area of application of the QSROA, we suggest a somewhat standardized setup of the hardware. The setup essentially revolves around the camera. We suggest that the camera and therefore the Jetson Nano is mounted at a fixed height of one meter above the counter. Further, we advise to place the camera in parallel to the surface of the counter. The Jetson itself needs to be connected to a local network via LAN. The packing area should preferably be well lit to avoid extensive shadows. There are no recommendations regarding the surface structure or the color of the counter.","title":"Hardware Setup"},{"location":"05_deployment/deployment/#docker","text":"Originally, we planned to do the complete developing process on other computers to subsequently build the Docker containers and upload them to a container registry. On the Jetson Nano one would then only call docker-compose to pull and start the containers. However, this concept led us into some problems as certain steps of our build process require the DeepStream and CUDA SDK to be present on the build system. The latter of which is only compatible with certain Nvidia GPUs. It may have been possible to build an adequate virtual machine to overcome the issue, anyhow the effort required for this would not have been justifiable. We therefore went with an adapted concept. This concept includes three distinct Docker Containers: one for the frontend, one for the backend and one for the inference. Now as before, both frontend and backend are built on PCs. By contrast, the inference container is being built on the Jetson Nano. In the following the containers are depicted in more detail. The creation of the container for the fronted starts with the conduction of a production build of the React application in a node.js container. Therefore, the needed packages ( package.json ) are installed and the source code of the application is copied into the container. The resulting production build is then copied into a nginx container, which we use to host the react website. nginx is a webserver software. For the backend, the source-files as well as the requirements.txt are copied into a python container. Then, the required packages are installed with pip. In a final step, the backend is executed via gunicorn, a http-server provider for python. Nvidia actually supplies a Docker base image for DeepStream , which is used for the inference. All necessary libraries are installed in the Docker Container and gst-python is being pulled and built from the GStreamer Github repo. Next, the files from the inference folder are copied into the container, the requirements.txt installed and run_server.py executed. run_server.py executes the deepstream pipeline and takes care of the Socket.IO connection to the backend.","title":"Docker"},{"location":"05_deployment/deployment/#gitlab-pipeline","text":"Originally it was planned to use a GitLab Runner for automated Docker builds. Merge requests in the master branch would trigger the pipeline to automatically build a new Docker container and upload it to the container registry. While the pipeline worked well for the frontend, the pushing already caused trouble. As the inference container must be built with scripts on the Jetson Nano either way, we decided to take another path. Thus, we ended up outright working with build scripts. Of the two alternatives, it came out as significantly less effortful. The fact that a limited number of people worked on this project in a brief time span supports this decision.","title":"GitLab Pipeline"},{"location":"05_deployment/deployment/#build-scripts","text":"As elaborated in the preceding paragraphs, both frontend and backend Docker containers are built on PCs. Unfortunately, KIT GitLab does not provide a container registry, so that we had to extend the project by a GitHub account, named \"aissgroup\". In the respective folders of frontend and backend, a script is available named deploy-arm.sh . These scripts build Docker Containers for the Jetson and push them to the container registry. For testing purposes on Windows, there are additional scripts available named deploy.sh . They differ in a way that the Docker Containers are now created for Windows. Unlike front- and backend, the inference must be built with scripts directly on the Jetson. Thus, the script named build.sh is to be executed on the device. The script creates a temporary build folder, in which all relevant files such as Github projects, model weights and labels are copied. The first thing built is the yolov5s.engine . Subsequently a libnvdsinfer_custom_impl_Yolo.so Library is built by the script. Finally, both files and labels.txt are copied into a folder named output. To finally deploy the whole service on the Jetson, two more scripts need to be used. They can be found on the top level of the project folder and are named build_all.sh and start_all.sh . Both of them must be executed on the Jetson. build_all.sh deletes all existing Docker Containers and images to be able to conduct a clean, new build and avoid accidental uses of old images. Following, both the frontend and the backend container get pulled from the repository and the inference Docker Container is being built. After successfully running the build script, the start_all.sh script is to be executed. The script stops all running containers and deletes them. It then starts the three containers of the QSROA application and exposes the following ports in the network: 5001 Backend Port 1234 Frontend Port 8554 RTSP Port It was decided to go with two separate scripts for the build and the execution as it enables repetitive starts of the application without rebuilding.","title":"Build Scripts"},{"location":"06_documentation/documentation/","text":"Documentation Pandoc For better collaboration in writing, correcting and following up on any changes we decided to write the documentation for this project on cloud-hosted word-documents. But because our final documentation has to be a markdown file we needed something to convert from word to markdown. For this task we use Pandoc . Installing Pandoc: pip install pandoc Converting word document to markdown: pandoc -f docx -t markdown your_file.docx -o your_file.md MkDocs MkDocs is a static site generator for building project documentation on a markdown basis. pip install mkdocs For a more appealing design, we chose Material for MkDocs . pip install mkdocs-material Because we also wanted to show videos in our documentation we installed the Plugin MkDocs Video . pip install mkdocs-video For converting our documentation into pdf we used the VSCode Plugin Markdown PDF . Commands : Feature: Command: Create a new project. mkdocs new [dir-name] Start the live-reloading docs server. mkdocs serve Build the documentation site. mkdocs build Print help message and exit. mkdocs -h","title":"Setup Documentation"},{"location":"06_documentation/documentation/#documentation","text":"","title":"Documentation"},{"location":"06_documentation/documentation/#pandoc","text":"For better collaboration in writing, correcting and following up on any changes we decided to write the documentation for this project on cloud-hosted word-documents. But because our final documentation has to be a markdown file we needed something to convert from word to markdown. For this task we use Pandoc . Installing Pandoc: pip install pandoc Converting word document to markdown: pandoc -f docx -t markdown your_file.docx -o your_file.md","title":"Pandoc"},{"location":"06_documentation/documentation/#mkdocs","text":"MkDocs is a static site generator for building project documentation on a markdown basis. pip install mkdocs For a more appealing design, we chose Material for MkDocs . pip install mkdocs-material Because we also wanted to show videos in our documentation we installed the Plugin MkDocs Video . pip install mkdocs-video For converting our documentation into pdf we used the VSCode Plugin Markdown PDF . Commands : Feature: Command: Create a new project. mkdocs new [dir-name] Start the live-reloading docs server. mkdocs serve Build the documentation site. mkdocs build Print help message and exit. mkdocs -h","title":"MkDocs"},{"location":"06_documentation/table_of_contributors/","text":"Table of Contributors The following table should give an overview about the competence fields of each contributor. The development of the final prototype and its documentation did not take place in separation of tasks, but more in a constant exchange and collaboration of all contributors, therefore a tabular division is difficult. Nevertheless, we try to give an overview of the core tasks of each contributor in the following but again it does not represent the elaboration of the QSROA. Task-Field Contributor(s) Use Case Felix, Magnus, Leon Data Generation Leon, Joshua, Magnus Model Magnus, Julian, Leon System Architecture Julian, Christian, Joshua Deployment Christian, Felix, Julian","title":"Table of Contributors"},{"location":"06_documentation/table_of_contributors/#table-of-contributors","text":"The following table should give an overview about the competence fields of each contributor. The development of the final prototype and its documentation did not take place in separation of tasks, but more in a constant exchange and collaboration of all contributors, therefore a tabular division is difficult. Nevertheless, we try to give an overview of the core tasks of each contributor in the following but again it does not represent the elaboration of the QSROA. Task-Field Contributor(s) Use Case Felix, Magnus, Leon Data Generation Leon, Joshua, Magnus Model Magnus, Julian, Leon System Architecture Julian, Christian, Joshua Deployment Christian, Felix, Julian","title":"Table of Contributors"}]}