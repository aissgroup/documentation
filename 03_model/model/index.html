<!-- Elements added to main will be displayed on all pages -->

<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="icon" href="../../assets/images/favicon.ico">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-7.1.9">
    
    
      
        <title>Model - Quick Service Restaurant Order Assistant - Team B Documentation</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.ca7ac06f.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.f1a3b89f.min.css">
        
          
          
          <meta name="theme-color" content="#009485">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
    
      


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="teal" data-md-color-accent="cyan">
  
    
    <script>function __prefix(e){return new URL("../..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#model" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Quick Service Restaurant Order Assistant - Team B Documentation" class="md-header__button md-logo" aria-label="Quick Service Restaurant Order Assistant - Team B Documentation" data-md-component="logo">
      
  <img src="../../assets/images/favicon.ico" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Quick Service Restaurant Order Assistant - Team B Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Model
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../.." class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../01_business_plan/use_case/" class="md-tabs__link">
        Use Case
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../02_data_generation/data_generation/" class="md-tabs__link">
        Data Generation
      </a>
    </li>
  

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="./" class="md-tabs__link md-tabs__link--active">
        Model
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../04_backend_frontend/system_architecture/" class="md-tabs__link">
        System Architecture
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../05_deployment/deployment/" class="md-tabs__link">
        Deployment
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../06_documentation/documentation/" class="md-tabs__link">
        Appendix
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Quick Service Restaurant Order Assistant - Team B Documentation" class="md-nav__button md-logo" aria-label="Quick Service Restaurant Order Assistant - Team B Documentation" data-md-component="logo">
      
  <img src="../../assets/images/favicon.ico" alt="logo">

    </a>
    Quick Service Restaurant Order Assistant - Team B Documentation
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      <label class="md-nav__link" for="__nav_2">
        Use Case
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Use Case" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Use Case
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../01_business_plan/use_case/" class="md-nav__link">
        Business Plan
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../01_business_plan/demo_video/" class="md-nav__link">
        QSROA - Demo Video
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      <label class="md-nav__link" for="__nav_3">
        Data Generation
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Data Generation" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Data Generation
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../02_data_generation/data_generation/" class="md-nav__link">
        Dataset
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" checked>
      
      <label class="md-nav__link" for="__nav_4">
        Model
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Model" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Model
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Model
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Model
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#model-architectures" class="md-nav__link">
    Model Architectures
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#yolo-architecture" class="md-nav__link">
    YOLO Architecture
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#yolo-model-comparison" class="md-nav__link">
    YOLO Model Comparison
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#about-yolov5s" class="md-nav__link">
    About YOLOv5(s)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#training-pipeline-pytorch-yolov5s" class="md-nav__link">
    Training-Pipeline PyTorch YOLOv5s
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#results" class="md-nav__link">
    Results
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      <label class="md-nav__link" for="__nav_5">
        System Architecture
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="System Architecture" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          System Architecture
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../04_backend_frontend/system_architecture/" class="md-nav__link">
        System Architecture
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" type="checkbox" id="__nav_6" >
      
      <label class="md-nav__link" for="__nav_6">
        Deployment
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Deployment" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          Deployment
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../05_deployment/deployment/" class="md-nav__link">
        Deployment
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7" type="checkbox" id="__nav_7" >
      
      <label class="md-nav__link" for="__nav_7">
        Appendix
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Appendix" data-md-level="1">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          Appendix
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../06_documentation/documentation/" class="md-nav__link">
        Setup Documentation
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../06_documentation/table_of_contributors/" class="md-nav__link">
        Table of Contributors
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#model-architectures" class="md-nav__link">
    Model Architectures
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#yolo-architecture" class="md-nav__link">
    YOLO Architecture
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#yolo-model-comparison" class="md-nav__link">
    YOLO Model Comparison
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#about-yolov5s" class="md-nav__link">
    About YOLOv5(s)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#training-pipeline-pytorch-yolov5s" class="md-nav__link">
    Training-Pipeline PyTorch YOLOv5s
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#results" class="md-nav__link">
    Results
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="model">Model</h1>
<p>Computer vision challenges were initially posed by simple geometric body
detections - tasks for which neural networks were not necessarily
needed. Due to the trend with increasing availability of computing power
and the establishment of Convolutional Neural Networks (CNNs), neural
networks in general, but especially CNNs in the domain Computer Vision
have gained publicity due to very good results. For this reason, we
decided to master our object detection using such neural networks -
available in the manner of open source in different model architectures.</p>
<h2 id="model-architectures">Model Architectures</h2>
<p>Since the requirements from the course are an object detection in
real-time on a Jetson Nano, we had to look for rather small neural
network architectures that perform high FPS rates (frames per second) on
relatively low processing power. For our use-case, real-time meant to be
a smooth detection at at least 8 FPS. The following figure shows
different model architectures deployed with NVIDIA's TensorRT
accelerator library on a Jetson Nano.</p>
<p><img alt="Deep Learning Inference Performance on Jetson Nano" src="https://developer.nvidia.com/sites/default/files/akamai/embedded/images/metrics/jetson_nano-deep_learning_inference_perf-chart.png" /></p>
<p>After further research, we decided either a model architecture from the
<a href="https://github.com/tensorflow/models/tree/master/research/object_detection">Tensorflow Object Detection
API</a>
or a YOLO architecture implemented with a widespread machine learning
library like <a href="https://pytorch.org/">PyTorch</a> would be the perfect fit,
since both are very well documented, and lack of time is one of our
biggest challenges.</p>
<p>With both libraries as possible solutions, we created training pipelines
to test several model architectures, such as the "SSD Mobilenet-V2" from
the Tensorflow Model Zoo, or a YOLOv4-CSP and evaluate their
performance.</p>
<p>Finally, we did not compare the performance of different YOLO model
architectures with models from the <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md">Tensorflow
Model-Zoo</a>,
because we decided to use Nvidia's <a href="https://developer.nvidia.com/deepstream-sdk">DeepStream
library</a>, which integrates
particularly well with YOLO architectures, for inference on the Jetson
Nano.</p>
<h2 id="yolo-architecture">YOLO Architecture</h2>
<p><a href="https://arxiv.org/pdf/1506.02640.pdf">YOLO</a>, an abbreviation for the
term "You Only Look Once", is a model architecture for object detection,
employing CNNs to detect in real-time. As the name indicates YOLO
architectures are single-stage object detectors.</p>
<p>In general, the architecture uses three techniques to achieve accurate
results with minimal error and a real-time detection.</p>
<p><strong>Residual Blocks:</strong><br>
The input-image is divided into grids of a quadratic dimension. Every
grid-cell will detect objects that appear within them.</p>
<p><strong>Bounding Box Regression:</strong><br>
The Bounding Box, as the rectangle outline of an object in an image,
consists of height and width, relative x and y center position and the
item-class. YOLO uses a single regression to align all Bounding Box
factors.</p>
<p><strong>Intersection over Union (IOU):</strong><br>
IOU is used by the YOLO architecture to provide a bounding box that fits
the object perfectly. Each grid-cell is responsible for predicting the
bounding box with its confidence score.</p>
<p><img alt="YOLO" src="../media/yolo.png" /></p>
<p>The above figure shows how the three techniques are applied to produce
the final object detection results.</p>
<h2 id="yolo-model-comparison">YOLO Model Comparison</h2>
<p>The DeepStream-based inference solution we built for the QSROA supports
the deployment of most common YOLO architectures with very little
reconfiguration effort. In order to further streamline our development
process, we decided to use a model supported by the
<a href="https://github.com/marcoslucianops/DeepStream-Yolo">DeepStream-Yolo</a>
project. As a result, we had several architectures from which we could
choose the best-suited model according to use case and hardware
restrictions.</p>
<ul>
<li>
<p><a href="https://pytorch.org/hub/ultralytics_yolov5/">YOLOv5</a></p>
<ul>
<li>
<p>available in various sizes</p>
</li>
<li>
<p>implements CSP Bottleneck like YOLOv4-CSP</p>
</li>
</ul>
</li>
<li>
<p>YOLOv4x-Mish</p>
</li>
<li>
<p>YOLOv4-CSP (Scaled/ <a href="https://arxiv.org/pdf/1911.11929.pdf">Cross-Stage Partial
    Networks</a>)</p>
<ul>
<li>
<p>code has some unknown issues with reproducing paper performance</p>
</li>
<li>
<p><a href="https://blog.roboflow.com/scaled-yolov4-tops-efficientdet/">outstanding
    performance</a></p>
</li>
<li>
<p>CSP architecture reduces inference computation</p>
</li>
</ul>
</li>
<li>
<p>YOLOv4</p>
<ul>
<li>in general, we can say YOLOv4 is <a href="https://github.com/AlexeyAB/darknet/issues/5920#issuecomment-642213028">an
    improvement</a>
    to YOLOv3</li>
</ul>
</li>
<li>
<p>YOLOv4-Tiny</p>
<ul>
<li>
<p>reduced network size compared to YOLOv4</p>
</li>
<li>
<p>Faster prediction but less accuracy</p>
</li>
</ul>
</li>
<li>
<p>YOLOv3-SPP (<a href="https://arxiv.org/ftp/arxiv/papers/1903/1903.08589.pdf">Spatial Pyramid
    Pooling</a>)</p>
<ul>
<li>useful especially for networks that have many different input
    image sizes - therefore a not necessary architecture for us</li>
</ul>
</li>
<li>
<p>YOLOv3</p>
</li>
<li>
<p><a href="https://github.com/WongKinYiu/PartialResidualNetworks">YOLOv3-Tiny-PRN</a> (Partial
    Residual Networks)</p>
<ul>
<li>
<p>has a more stable performance than YOLOv3-Tiny</p>
</li>
<li>
<p>better FPS rate than YOLO-Tiny</p>
</li>
</ul>
</li>
<li>
<p>YOLOv3-Tiny</p>
</li>
<li>
<p>YOLOv3-Lite</p>
</li>
<li>
<p>YOLOv3-Nano</p>
</li>
<li>
<p>YOLO-Fastest</p>
</li>
<li>
<p>YOLO-Fastest-XL</p>
</li>
<li>
<p>YOLOv2</p>
</li>
<li>
<p>YOLOv2-Tiny</p>
</li>
</ul>
<p>Our selection is based on detection performance, speed (FPS) and
scalability. By scalability, we refer to the ability to scale a model in
size without requiring major changes to the model architecture. After
evaluating several model architectures we decided to have a deeper
comparison of the <strong>YOLOv4-CSP</strong> and <strong>YOLOv5s</strong>. Therefore, we balanced
the pros and contra of both models.</p>
<table>
<thead>
<tr>
<th>YOLOv4-CSP</th>
<th>Pro:</th>
<th>Contra:</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Research Paper available</td>
<td>Low FPS (3.9FPS) or image-stutter</td>
</tr>
<tr>
<td></td>
<td>High mAP</td>
<td>Possible overfitting because of synthetic data</td>
</tr>
<tr>
<td></td>
<td>Native convert.</td>
<td>Code has some unknown issues with reproducing paper performance</td>
</tr>
</tbody>
</table>
<p>The following video is a YOLOv4-CSP trained on the COCO2017 Dataset
deployed on the Jetson Nano. Without the 5 skip frames that we apply, we
achieve a 3.9 FPS-rate in average on the Jetson Nano, which visualized
looks like a stutter from frame to frame.</p>
<p><iframe src="../media/yolov4csp.mp4" style="position: relative; weight: 100%; height: 18.7225vw; width: 50vw" frameborder="0" allowfullscreen></iframe></p>
<table>
<thead>
<tr>
<th>YOLOv5s</th>
<th>Pro:</th>
<th>Contra:</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>High FPS (12FPS)</td>
<td>No paper available</td>
</tr>
<tr>
<td></td>
<td>Potentially less overfitting</td>
<td>"non-transparent" python code</td>
</tr>
<tr>
<td></td>
<td>Fast training time</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Less memory usage</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Performance reserve because of less memory usage</td>
<td></td>
</tr>
</tbody>
</table>
<p><iframe src="../media/yolov5s.mp4" style="position: relative; weight: 100%; height: 18.7225vw; width: 50vw" frameborder="0" allowfullscreen></iframe></p>
<p>This video shows the YOLOv5s trained on the COCO2017 Dataset, also
deployed on the Jetson Nano, without any stutter (1 skip frame; 13 FPS),
but as you can see with a slightly worse detection performance than the
YOLOv4-CSP.</p>
<p>After a long evaluation and weighing up various characteristics we
decided <strong>YOLOv5s (<a href="https://github.com/ultralytics/yolov5/releases/tag/v5.0">Release
5.0</a>)</strong>, the
smallest <a href="https://pytorch.org/hub/ultralytics_yolov5/">YOLOv5</a> model
would be the most suitable one for our use case. Especially due to the
high scalability and fast training time - both properties that fit very
well to the adaptivity of our approach of the synthetic dataset to
changing conditions and thus perfectly match the high flexibility we
approach to e.g. sessional changes described in the business plan.</p>
<h2 id="about-yolov5s">About YOLOv5(s)</h2>
<p>YOLOv5 is a GitHub project developed and maintained by the artificial
intelligence startup <a href="https://ultralytics.com/">Ultralytics</a>. The
project describes itself as a family of object detection architectures
and models pretrained on the COCO dataset. While as of now there is no
research paper available, it can still be seen as an unofficial
successor to YOLOv3 as it incorporates various architectural
advancements and new data augmentation techniques for training such as
mosaic augmentation. The project is under active development and since
the original release in June of 2020, the architecture was updated
multiple times to further improve performance. Contrary to the original
YOLO, YOLOv5 is implemented using the popular PyTorch framework.</p>
<p>YOLOv5 is available in four different sizes. The different sizes with
corresponding characteristics are listed in the following table.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Size (MB)</th>
<th>FPS-rate (ms)</th>
<th>mAP (COCO)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>YOLOv5s</strong></td>
<td><strong>14</strong></td>
<td><strong>2.0</strong></td>
<td><strong>37.2</strong></td>
</tr>
<tr>
<td>YOLOv5m</td>
<td>41</td>
<td>2.7</td>
<td>44.5</td>
</tr>
<tr>
<td>YOLOv5l</td>
<td>90</td>
<td>3.8</td>
<td>48.2</td>
</tr>
<tr>
<td>YOLOv5x</td>
<td>168</td>
<td>6.1</td>
<td>50.4</td>
</tr>
</tbody>
</table>
<p>Since the characteristics of the smallest model (YOLOv5s) fit our
requirements regarding detection speed and size very well, we will focus
more on its technical details in the following (the other listed
versions only differ slightly in model layers and number of parameters).</p>
<p>The YOLOv5s, due to its single-stage object detection characteristic
also consists of three main parts. To extract important features from
the input image the model backbone is a <a href="https://github.com/WongKinYiu/CrossStagePartialNetworks">Cross Stage Partial
network</a> (CSP).
For better model generalization the model neck is used to generate
feature pyramids like the <a href="https://arxiv.org/pdf/1803.01534.pdf">PANet</a>.
The final detection is done in the model head. Bounding Boxes, class
probabilities, output vector and the objectness scores are generated on
the given features.</p>
<h2 id="training-pipeline-pytorch-yolov5s">Training-Pipeline PyTorch YOLOv5s</h2>
<p>The programmed python script is designed to run on the BWUniCluster
because Image-Processing requires high processing power which is often
limited on personal computers. With limitations on Google Colab like
restricting training time and computing power, we decided to use the
Cluster instead.</p>
<p>The script used for training can be found under <code>./model/train/start.sh</code>.</p>
<p>It is a simple shell script executing the training scripts found in the
repository of the original Yolov5 model. To run the training, place the
<code>start.sh</code> script inside a clone of the Yolov5 repository:</p>
<p><a href="https://github.com/ultralytics/yolov5">https://github.com/ultralytics/yolov5</a></p>
<p>Make sure the <code>--data</code> parameter links to the right dataset. In our
case the synthetic dataset we used for training:</p>
<p><a href="https://drive.google.com/file/d/116fG7o5RiESG0x1h3kxuD69LcGa73jym/view?usp=sharing">https://drive.google.com/file/d/116fG7o5RiESG0x1h3kxuD69LcGa73jym/view?usp=sharing</a></p>
<p>Executing the script will train a model from pretrained weights. Since
we used a total of 4 GPUs on the cluster, the distributed launch
parameter is enabled. Adapt this parameter according to the resources
you have available. If your GPU runs out of memory, decrease the batch
size to a size that works with your setup.</p>
<p>After training, the script converts the model into TensorRT format. This
ensures optimal performance when deploying the model to the Jetson Nano.</p>
<p>Statistics about your training run can be found in the newly created
<code>./runs/train</code> folder.</p>
<h2 id="results">Results</h2>
<p>The model was trained for 100 epochs on synthetic data only. For
training the 'finetuning' hyperparameters provided by YOLOv5s were used.</p>
<p>The model was not trained from scratch but from weights provided by
YOLOv5s. These weights were pretrained on the
<a href="https://arxiv.org/pdf/1405.0312.pdf">COCO</a> dataset. Overall, the
training took just over 8 hours on 4 Nvidia Tesla V100 GPUs.</p>
<p>While convergence on the validation data could be observed around the 80
epochs mark, some further hyperparameter tuning might further increase
model performance.</p>
<p><img alt="Training" src="../media/training.png" /></p>
<p>After 100 epochs the model performs very well on the validation dataset.
Keep in mind that the validation dataset consists of only synthetic data
as well. Surprisingly despite some heavy regularization the model still
achieves very high accuracies. In many cases, the regularization
techniques are so intense that recognizing all items in the images is
challenging even to humans. The model seems to achieve similar or even
performance than a human would on the synthetic dataset.</p>
<p><img alt="Confusion Matrix Synthetic dataset" src="../media/confusion_matrix_syn.png" /></p>
<p>The final performance of the model was evaluated on a test dataset -
introduced in the dataset-section never seen before by the model. In
contrast to the validation dataset, the test dataset consists of real
images only.</p>
<p>Keep in mind that the test data contains highly complex data, as
explained in the dataset section. The performance figures here therefore
should be considered as a lower bound rather than a final model
performance.</p>
<p>Generally, the model recognized all the classes in the test dataset with
high accuracy. As expected, the confusion matrix shows the highest
accuracy in larger item-classes with very distinct features, such as the
packaging of Big Macs and the McChicken.</p>
<p>Problems are found with products that naturally change depending on the
environment as well as between products. A good example is the low
accuracy of sundaes. Sundaes come in a variety of different dessert
sauces, each one stirred slightly different into the ice cream, making
every sundae look unique. Additionally, ice cream tends to melt rather
quickly, so when capturing images for the test dataset the appearance
will change continuously. While this was a challenge for us, it is less
critical for real life situations in the restaurant, as the ice is
freshly prepared.</p>
<p>The high accuracy achieved on recognizing fries shows that these issues
can be overcome with more training data. While fries have a high
variability within their class, we could create a relatively large
amount of training data for them by simply rearranging the fries. This
seems to have greatly increased models' understanding. Our hypothesis
is that with a sufficiently large amount of training data these issues
could also be overcome for other classes.</p>
<p><img alt="Confusion Matrix Test dataset" src="../media/confusion_matrix_tes.png" /></p>
<p>When investigating the models' results, the model seemed to have issues
with items being partly out of frame or overlapped by other items. This
is especially the case when moving items into the frame or out of the
frame. While this issue will decrease the prediction accuracy of the
model on our test dataset, in a real world application this has little
to no significance. Since the system is observing a continuous stream of
images, missing the item for a couple of frames while it is being moved
into frame or blocked partially by a hand should not have any
consequences. In many cases, the model will be able to recognize the
item once it's fully in frame, and the logic we implemented in the
backend of our system is able to filter for items that disappear for
single frames or are falsely recognized for short time. By averaging
over a longer timeframe, we can greatly reduce the models' false
recognitions, as illustrated in the following figure.</p>
<p><img alt="Resilience on Test Data" src="../media/test__data.png" /></p>
                
              
              
                


              
            </article>
          </div>
        </div>
        
      </main>
      
<footer class="md-footer">
    <div class="md-footer-meta md-typeset">
        <div class="md-footer-meta__inner md-grid">
            <!-- Copyright and theme information -->
            <div class="md-footer-copyright">
                
                <div class="md-footer-copyright__highlight">
                    With ❤ by Joshua, Leon, Julian, Felix, Christian & Magnus
                </div>
                
                powered by
                <a href="https://www.mkdocs.org" title="MkDocs">MkDocs</a>
                and
                <a href="https://squidfunk.github.io/mkdocs-material/"
                   title="Material for MkDocs">
                    Material for MkDocs</a>
            </div>
            
            
            
        </div>
    </div>
</footer>

    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.tabs.sticky"], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../../assets/javascripts/workers/search.477d984a.min.js", "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.82b56eb2.min.js"></script>
      
    
  </body>
</html>